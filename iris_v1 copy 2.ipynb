{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthi/.local/lib/python3.10/site-packages/gym/envs/registration.py:727: DeprecationWarning: The package name gym_robotics has been deprecated in favor of gymnasium_robotics. Please uninstall gym_robotics and install gymnasium_robotics with `pip install gymnasium_robotics`. Future releases will be maintained under the new package name gymnasium_robotics.\n",
      "  fn()\n",
      "/usr/lib/python3/dist-packages/pythran/config.py:8: DeprecationWarning: \n",
      "\n",
      "  `numpy.distutils` is deprecated since NumPy 1.23.0, as a result\n",
      "  of the deprecation of `distutils` itself. It will be removed for\n",
      "  Python >= 3.12. For older Python versions it will remain present.\n",
      "  It is recommended to use `setuptools < 60.0` for those Python versions.\n",
      "  For more details, see:\n",
      "    https://numpy.org/devdocs/reference/distutils_status_migration.html \n",
      "\n",
      "\n",
      "  import numpy.distutils.system_info as numpy_sys\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/setuptools/_distutils/msvccompiler.py:66: DeprecationWarning: msvccompiler is deprecated and slated to be removed in the future. Please discontinue use or file an issue with pypa/distutils describing your use case.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pythran/tables.py:4520: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, method):\n",
      "/usr/lib/python3/dist-packages/pythran/tables.py:4553: FutureWarning: In the future `np.bytes` will be defined as the corresponding NumPy scalar.\n",
      "  obj = getattr(themodule, elem)\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/setuptools/sandbox.py:13: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/pkg_resources/__init__.py:2846: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/numpy/distutils/command/build_ext.py:8: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer_group\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/numpy/distutils/command/build_clib.py:11: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer_group\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/numpy/distutils/command/build_src.py:10: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer_group, newer\n",
      "/home/keerthi/.local/lib/python3.10/site-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
      "  from distutils.dep_util import newer, newer_group\n",
      "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Nov 28 2023 23:45:17\n",
      "load datafile: 100%|██████████| 8/8 [00:00<00:00, 30.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "env = gym.make(\"maze2d-open-v0\")\n",
    "dataset = env.get_dataset()\n",
    "print(dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_into_trajectories(dataset):\n",
    "    observations = dataset['observations']\n",
    "    actions = dataset['actions']\n",
    "    rewards = dataset['rewards']\n",
    "    # 'dones' in some datasets are called 'terminals'\n",
    "    dones = dataset['terminals']\n",
    "\n",
    "    trajectories = []\n",
    "    current_trajectory = {\n",
    "        'observations': [],\n",
    "        'actions': [],\n",
    "        'rewards': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(observations)):\n",
    "        # Append the current timestep's data to the current trajectory\n",
    "        current_trajectory['observations'].append(observations[i])\n",
    "        current_trajectory['actions'].append(actions[i])\n",
    "        current_trajectory['rewards'].append(rewards[i])\n",
    "\n",
    "        # If the 'done' flag is True, the current trajectory ends\n",
    "        if rewards[i] == 1:\n",
    "            # Convert lists to numpy arrays\n",
    "            current_trajectory['observations'] = np.array(\n",
    "                current_trajectory['observations'])\n",
    "            current_trajectory['actions'] = np.array(\n",
    "                current_trajectory['actions'])\n",
    "            current_trajectory['rewards'] = np.array(\n",
    "                current_trajectory['rewards'])\n",
    "\n",
    "            # Add the current trajectory to the list of trajectories\n",
    "            trajectories.append(current_trajectory)\n",
    "\n",
    "            # Reset the current trajectory\n",
    "            current_trajectory = {\n",
    "                'observations': [],\n",
    "                'actions': [],\n",
    "                'rewards': []\n",
    "            }\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "# Split the dataset into a list of trajectories\n",
    "trajectories = split_into_trajectories(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/137777 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3784/137777 [00:23<11:08, 200.40it/s]/home/keerthi/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 33%|███▎      | 46031/137777 [04:37<10:02, 152.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46000: Avg Reward: 0.0000, Avg Policy Loss: 0.4679, Avg VAE Loss: 0.3262, Avg Value Loss: 30.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 55034/137777 [05:34<05:54, 233.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55000: Avg Reward: 0.0000, Avg Policy Loss: 0.4531, Avg VAE Loss: 0.0369, Avg Value Loss: 380.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 96024/137777 [09:26<08:24, 82.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96000: Avg Reward: 0.0000, Avg Policy Loss: 0.4296, Avg VAE Loss: 0.0482, Avg Value Loss: 1387.3904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 106058/137777 [10:32<02:50, 185.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 106000: Avg Reward: 0.0000, Avg Policy Loss: 0.4296, Avg VAE Loss: 0.0519, Avg Value Loss: 1087.1723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137777/137777 [13:32<00:00, 169.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "Run this training loop for full trajecory from dataset'''\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, trajectories, trajectory_length=5):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "    iteration = 0\n",
    "    for trajectory in tqdm(trajectories):\n",
    "        # Move data to GPU\n",
    "        iteration += 1\n",
    "        states = torch.tensor(\n",
    "            trajectory['observations'], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            trajectory['actions'], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            trajectory['rewards'], dtype=torch.float32).to(device)\n",
    "        if len(rewards) <= 1:\n",
    "            continue\n",
    "        if actions.dim() <= 1:\n",
    "            continue\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-1]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 1000 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20595/3465970709.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Step 20: Goal reached!\n",
      "Episode 2, Step 24: Goal reached!\n",
      "Episode 4, Step 26: Goal reached!\n",
      "Episode 5, Step 27: Goal reached!\n",
      "Episode 6, Step 26: Goal reached!\n",
      "Episode 8, Step 38: Goal reached!\n",
      "Episode 10, Step 26: Goal reached!\n",
      "Video saved to env_policy_video_3.mp4\n",
      "Accuracy is 70.0000\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_policy_as_video(low_level_policy, goal_proposal_vae, value_function, env, num_episodes=10, max_steps=10000, save_path=\"env_policy_video_3.mp4\"):\n",
    "    # Define video writer using OpenCV\n",
    "    height, width, _ = env.render(mode=\"rgb_array\").shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "    video_writer = cv2.VideoWriter(save_path, fourcc, 30, (width, height))\n",
    "    accuracy = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = state.copy()  # Assuming the goal is part of the observation for simplicity\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Render the environment and capture the frame\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            # Write frame to video\n",
    "            video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "            # VAE update\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "            z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "            goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "            goal_final = goal\n",
    "            value = value_function(goal, torch.squeeze(\n",
    "                low_level_policy(state_tensor, goal)))\n",
    "            for k in range(5):\n",
    "                z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "                goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "                if value_function(goal, torch.squeeze(low_level_policy(state_tensor, goal))) > value:\n",
    "                    goal_final = goal\n",
    "                    value = value_function(\n",
    "                        goal, torch.squeeze(low_level_policy(state_tensor, goal)))\n",
    "            goal = goal_final\n",
    "            next_state = None\n",
    "            # Get the action from the low-level policy\n",
    "            reward = 0\n",
    "            for _ in range(2):\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action = low_level_policy(\n",
    "                    state_tensor, goal).detach().numpy()\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                action = np.squeeze(action)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    print(\n",
    "                        f\"Episode {episode+1}, Step {step+1}: Goal reached!\")\n",
    "                    break\n",
    "                if done:\n",
    "                    break  # Terminate the episode if done is True\n",
    "            if done:\n",
    "                break\n",
    "            if reward > 0:\n",
    "                accuracy += 1\n",
    "                break\n",
    "    # Release the video writer after finishing\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {save_path}\")\n",
    "    print(f\"Accuracy is {accuracy/num_episodes*100.0:.4f}\")\n",
    "\n",
    "\n",
    "# Visualize the learned policy as a video\n",
    "visualize_policy_as_video(low_level_policy.to(\n",
    "    \"cpu\"), goal_proposal_vae.to(\"cpu\"), value_network.to(\"cpu\"), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(low_level_policy.state_dict(),\n",
    "           \"/home/keerthi/IRIS/low_level_policy_full_traj.pth\")\n",
    "torch.save(goal_proposal_vae.state_dict(),\n",
    "           \"/home/keerthi/IRIS/goal_proposal_cvae_full_traj.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
