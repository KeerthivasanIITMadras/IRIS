{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:00<00:00, 32.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "env = gym.make(\"maze2d-umaze-v1\")\n",
    "dataset = env.get_dataset()\n",
    "print(dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_IRIS(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=100000, trajectory_length=7):\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 10\n",
    "#     gamma = 0.99\n",
    "#     # Assume observations are organized by trajectories\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     for iteration in range(num_iterations):\n",
    "#         # Sample a trajectory index (rather than random individual steps)\n",
    "#         trajectory_idx = np.random.choice(num_trajectories)\n",
    "\n",
    "#         # Define start and end indices for the trajectory\n",
    "#         start_idx = trajectory_idx * trajectory_length\n",
    "#         end_idx = start_idx + trajectory_length\n",
    "\n",
    "#         # Sample the entire trajectory (states, actions, goals, rewards)\n",
    "#         states = torch.tensor(\n",
    "#             dataset['observations'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = torch.tensor(\n",
    "#             dataset['actions'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         goals = torch.tensor(dataset['infos/goal']\n",
    "#                              [start_idx:end_idx], dtype=torch.float32)\n",
    "#         rewards = torch.tensor(\n",
    "#             dataset['rewards'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "#         # Perform the sampling operation\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg+gamma*max_value.detach()\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         if iteration % 1000 == 0:\n",
    "#             print(\n",
    "#                 f\"Iteration {iteration}: VAE Loss: {vae_loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 20\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# train_IRIS(low_level_policy, goal_proposal_vae,\n",
    "#            action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# def train_IRIS(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=20000, trajectory_length=50):\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 10\n",
    "#     gamma = 0.99\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     # Variables to store cumulative statistics\n",
    "#     cumulative_rewards = []\n",
    "#     cumulative_policy_loss = []\n",
    "#     cumulative_vae_loss = []\n",
    "#     cumulative_value_loss = []\n",
    "\n",
    "#     for iteration in range(num_iterations):\n",
    "#         trajectory_idx = np.random.choice(num_trajectories)\n",
    "#         start_idx = trajectory_idx * trajectory_length\n",
    "#         end_idx = start_idx + trajectory_length\n",
    "\n",
    "#         states = torch.tensor(\n",
    "#             dataset['observations'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = torch.tensor(\n",
    "#             dataset['actions'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         rewards = torch.tensor(\n",
    "#             dataset['rewards'][start_idx:end_idx], dtype=torch.float32)\n",
    "\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "\n",
    "#         # Perform sampling and value update\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg + gamma * max_value.detach()\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "#         # Update optimizers\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         # Store losses and reward\n",
    "#         cumulative_rewards.append(reward_sg.item())\n",
    "#         cumulative_policy_loss.append(policy_loss.item())\n",
    "#         cumulative_vae_loss.append(vae_loss.item())\n",
    "#         cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "#         # Print averages every 1000 iterations\n",
    "#         if iteration % 1000 == 0 and iteration > 0:\n",
    "#             avg_reward = np.mean(cumulative_rewards)\n",
    "#             avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "#             avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "#             avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "#             print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "#                   f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "#                   f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "#                   f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "#             # Reset cumulative statistics\n",
    "#             cumulative_rewards = []\n",
    "#             cumulative_policy_loss = []\n",
    "#             cumulative_vae_loss = []\n",
    "#             cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 20\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# train_IRIS(low_level_policy, goal_proposal_vae,\n",
    "#            action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000: Avg Reward: 0.0929, Avg Policy Loss: 0.5161, Avg VAE Loss: 0.5435, Avg Value Loss: 0.4712\n",
      "Iteration 2000: Avg Reward: 0.0440, Avg Policy Loss: 0.4861, Avg VAE Loss: 0.1007, Avg Value Loss: 0.1934\n",
      "Iteration 3000: Avg Reward: 0.0730, Avg Policy Loss: 0.4603, Avg VAE Loss: 0.0830, Avg Value Loss: 3.1286\n",
      "Iteration 4000: Avg Reward: 0.0720, Avg Policy Loss: 0.4362, Avg VAE Loss: 0.0933, Avg Value Loss: 1.4655\n",
      "Iteration 5000: Avg Reward: 0.1060, Avg Policy Loss: 0.4029, Avg VAE Loss: 0.0559, Avg Value Loss: 0.8981\n",
      "Iteration 6000: Avg Reward: 0.0950, Avg Policy Loss: 0.3719, Avg VAE Loss: 0.0719, Avg Value Loss: 0.3152\n",
      "Iteration 7000: Avg Reward: 0.0640, Avg Policy Loss: 0.3455, Avg VAE Loss: 0.0569, Avg Value Loss: 0.2787\n",
      "Iteration 8000: Avg Reward: 0.0860, Avg Policy Loss: 0.3179, Avg VAE Loss: 0.0506, Avg Value Loss: 0.4265\n",
      "Iteration 9000: Avg Reward: 0.0610, Avg Policy Loss: 0.2955, Avg VAE Loss: 0.0410, Avg Value Loss: 0.3590\n",
      "Iteration 10000: Avg Reward: 0.0720, Avg Policy Loss: 0.2762, Avg VAE Loss: 0.0415, Avg Value Loss: 1.1706\n",
      "Iteration 11000: Avg Reward: 0.0850, Avg Policy Loss: 0.2612, Avg VAE Loss: 0.0516, Avg Value Loss: 0.4367\n",
      "Iteration 12000: Avg Reward: 0.1100, Avg Policy Loss: 0.2485, Avg VAE Loss: 0.0366, Avg Value Loss: 0.5035\n",
      "Iteration 13000: Avg Reward: 0.0810, Avg Policy Loss: 0.2282, Avg VAE Loss: 0.0326, Avg Value Loss: 0.7752\n",
      "Iteration 14000: Avg Reward: 0.0850, Avg Policy Loss: 0.2231, Avg VAE Loss: 0.0352, Avg Value Loss: 0.3579\n",
      "Iteration 15000: Avg Reward: 0.0820, Avg Policy Loss: 0.2119, Avg VAE Loss: 0.0272, Avg Value Loss: 0.4822\n",
      "Iteration 16000: Avg Reward: 0.0980, Avg Policy Loss: 0.2005, Avg VAE Loss: 0.0413, Avg Value Loss: 1.1622\n",
      "Iteration 17000: Avg Reward: 0.0750, Avg Policy Loss: 0.1929, Avg VAE Loss: 0.0305, Avg Value Loss: 1.1917\n",
      "Iteration 18000: Avg Reward: 0.0630, Avg Policy Loss: 0.1978, Avg VAE Loss: 0.0464, Avg Value Loss: 1.4136\n",
      "Iteration 19000: Avg Reward: 0.1100, Avg Policy Loss: 0.1853, Avg VAE Loss: 0.0354, Avg Value Loss: 2.6705\n",
      "Iteration 20000: Avg Reward: 0.0630, Avg Policy Loss: 0.1884, Avg VAE Loss: 0.0223, Avg Value Loss: 0.7251\n",
      "Iteration 21000: Avg Reward: 0.0530, Avg Policy Loss: 0.1848, Avg VAE Loss: 0.0357, Avg Value Loss: 0.7267\n",
      "Iteration 22000: Avg Reward: 0.1040, Avg Policy Loss: 0.1866, Avg VAE Loss: 0.0223, Avg Value Loss: 0.6737\n",
      "Iteration 23000: Avg Reward: 0.0830, Avg Policy Loss: 0.1868, Avg VAE Loss: 0.0284, Avg Value Loss: 3.3513\n",
      "Iteration 24000: Avg Reward: 0.1230, Avg Policy Loss: 0.1827, Avg VAE Loss: 0.0330, Avg Value Loss: 3.2787\n",
      "Iteration 25000: Avg Reward: 0.0950, Avg Policy Loss: 0.1846, Avg VAE Loss: 0.0198, Avg Value Loss: 2.4309\n",
      "Iteration 26000: Avg Reward: 0.0910, Avg Policy Loss: 0.1833, Avg VAE Loss: 0.0338, Avg Value Loss: 4.3234\n",
      "Iteration 27000: Avg Reward: 0.0860, Avg Policy Loss: 0.1803, Avg VAE Loss: 0.0318, Avg Value Loss: 10.9422\n",
      "Iteration 28000: Avg Reward: 0.1160, Avg Policy Loss: 0.1825, Avg VAE Loss: 0.0195, Avg Value Loss: 6.4188\n",
      "Iteration 29000: Avg Reward: 0.0900, Avg Policy Loss: 0.1853, Avg VAE Loss: 0.0226, Avg Value Loss: 3.4828\n",
      "Iteration 30000: Avg Reward: 0.0710, Avg Policy Loss: 0.1785, Avg VAE Loss: 0.0271, Avg Value Loss: 2.1380\n",
      "Iteration 31000: Avg Reward: 0.0840, Avg Policy Loss: 0.1843, Avg VAE Loss: 0.0147, Avg Value Loss: 4.3842\n",
      "Iteration 32000: Avg Reward: 0.1000, Avg Policy Loss: 0.1823, Avg VAE Loss: 0.0273, Avg Value Loss: 2.8346\n",
      "Iteration 33000: Avg Reward: 0.0930, Avg Policy Loss: 0.1832, Avg VAE Loss: 0.0243, Avg Value Loss: 1.9897\n",
      "Iteration 34000: Avg Reward: 0.1030, Avg Policy Loss: 0.1808, Avg VAE Loss: 0.0197, Avg Value Loss: 0.6117\n",
      "Iteration 35000: Avg Reward: 0.0310, Avg Policy Loss: 0.1741, Avg VAE Loss: 0.0115, Avg Value Loss: 0.3384\n",
      "Iteration 36000: Avg Reward: 0.0940, Avg Policy Loss: 0.1790, Avg VAE Loss: 0.0220, Avg Value Loss: 7.9011\n",
      "Iteration 37000: Avg Reward: 0.0740, Avg Policy Loss: 0.1812, Avg VAE Loss: 0.0247, Avg Value Loss: 17.2107\n",
      "Iteration 38000: Avg Reward: 0.0800, Avg Policy Loss: 0.1831, Avg VAE Loss: 0.0202, Avg Value Loss: 5.7711\n",
      "Iteration 39000: Avg Reward: 0.0830, Avg Policy Loss: 0.1896, Avg VAE Loss: 0.0315, Avg Value Loss: 2.6847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def train_IRIS(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=40000, trajectory_length=5):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        trajectory_idx = iteration\n",
    "        start_idx = trajectory_idx * trajectory_length\n",
    "        end_idx = start_idx + trajectory_length\n",
    "\n",
    "        # Move data to GPU\n",
    "        states = torch.tensor(\n",
    "            dataset['observations'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            dataset['actions'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            dataset['rewards'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-2]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 1000 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS(low_level_policy, goal_proposal_vae,\n",
    "           action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4307/2376817933.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Step 26: Goal reached!\n",
      "Video saved to env_policy_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_policy_as_video(low_level_policy, env, num_episodes=1, max_steps=10000, save_path=\"env_policy_video.mp4\"):\n",
    "    # Define video writer using OpenCV\n",
    "    height, width, _ = env.render(mode=\"rgb_array\").shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "    video_writer = cv2.VideoWriter(save_path, fourcc, 30, (width, height))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = state.copy()  # Assuming the goal is part of the observation for simplicity\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Render the environment and capture the frame\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            # Write frame to video\n",
    "            video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "            # VAE update\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "            z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "            goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "\n",
    "            next_state = None\n",
    "            # Get the action from the low-level policy\n",
    "            reward = 0\n",
    "            for _ in range(3):\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action = low_level_policy(\n",
    "                    state_tensor, goal).detach().numpy()\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                action = np.squeeze(action)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    print(\n",
    "                        f\"Episode {episode+1}, Step {step+1}: Goal reached!\")\n",
    "                    break\n",
    "                if done:\n",
    "                    break  # Terminate the episode if done is True\n",
    "            if done:\n",
    "                break\n",
    "            if reward > 0:\n",
    "                break\n",
    "    # Release the video writer after finishing\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {save_path}\")\n",
    "\n",
    "\n",
    "# Visualize the learned policy as a video\n",
    "visualize_policy_as_video(low_level_policy, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
