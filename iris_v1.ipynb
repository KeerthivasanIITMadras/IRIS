{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:00<00:00, 33.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "env = gym.make(\"maze2d-umaze-v1\")\n",
    "dataset = env.get_dataset()\n",
    "print(dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_into_trajectories(dataset):\n",
    "    observations = dataset['observations']\n",
    "    actions = dataset['actions']\n",
    "    rewards = dataset['rewards']\n",
    "    # 'dones' in some datasets are called 'terminals'\n",
    "    dones = dataset['terminals']\n",
    "\n",
    "    trajectories = []\n",
    "    current_trajectory = {\n",
    "        'observations': [],\n",
    "        'actions': [],\n",
    "        'rewards': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(observations)):\n",
    "        # Append the current timestep's data to the current trajectory\n",
    "        current_trajectory['observations'].append(observations[i])\n",
    "        current_trajectory['actions'].append(actions[i])\n",
    "        current_trajectory['rewards'].append(rewards[i])\n",
    "\n",
    "        # If the 'done' flag is True, the current trajectory ends\n",
    "        if rewards[i] == 1:\n",
    "            # Convert lists to numpy arrays\n",
    "            current_trajectory['observations'] = np.array(\n",
    "                current_trajectory['observations'])\n",
    "            current_trajectory['actions'] = np.array(\n",
    "                current_trajectory['actions'])\n",
    "            current_trajectory['rewards'] = np.array(\n",
    "                current_trajectory['rewards'])\n",
    "\n",
    "            # Add the current trajectory to the list of trajectories\n",
    "            trajectories.append(current_trajectory)\n",
    "\n",
    "            # Reset the current trajectory\n",
    "            current_trajectory = {\n",
    "                'observations': [],\n",
    "                'actions': [],\n",
    "                'rewards': []\n",
    "            }\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "# Split the dataset into a list of trajectories\n",
    "trajectories = split_into_trajectories(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81254"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'observations': array([[ 1.08564889e+00,  1.97457337e+00,  9.81034897e-03,\n",
       "          2.17442401e-02],\n",
       "        [ 1.08439267e+00,  1.97413003e+00, -1.25623643e-01,\n",
       "         -4.43378054e-02],\n",
       "        [ 1.08075774e+00,  1.97527540e+00, -3.63488287e-01,\n",
       "          1.14539884e-01],\n",
       "        [ 1.07474983e+00,  1.97879970e+00, -6.00786448e-01,\n",
       "          3.52430940e-01],\n",
       "        [ 1.06738663e+00,  1.98469722e+00, -7.36318588e-01,\n",
       "          5.89755416e-01],\n",
       "        [ 1.06051731e+00,  1.99138772e+00, -6.86936378e-01,\n",
       "          6.69048548e-01],\n",
       "        [ 1.05162358e+00,  2.00044394e+00, -8.89374971e-01,\n",
       "          9.05618966e-01],\n",
       "        [ 1.04106534e+00,  2.01186013e+00, -1.05581987e+00,\n",
       "          1.14162600e+00],\n",
       "        [ 1.03120279e+00,  2.02563095e+00, -9.86252546e-01,\n",
       "          1.37707090e+00],\n",
       "        [ 1.02209210e+00,  2.04175043e+00, -9.11072493e-01,\n",
       "          1.61195505e+00],\n",
       "        [ 1.01258969e+00,  2.06021333e+00, -9.50245380e-01,\n",
       "          1.84627974e+00],\n",
       "        [ 1.00443995e+00,  2.08064246e+00, -8.14975142e-01,\n",
       "          2.04291296e+00],\n",
       "        [ 9.96554136e-01,  2.10252571e+00, -7.88576186e-01,\n",
       "          2.18834519e+00],\n",
       "        [ 9.89647985e-01,  2.12673879e+00, -6.90616190e-01,\n",
       "          2.42129707e+00],\n",
       "        [ 9.83179212e-01,  2.15327573e+00, -6.46880388e-01,\n",
       "          2.65369439e+00],\n",
       "        [ 9.78958547e-01,  2.18052292e+00, -4.22065824e-01,\n",
       "          2.72471571e+00],\n",
       "        [ 9.73251224e-01,  2.20792222e+00, -5.70733249e-01,\n",
       "          2.73992705e+00],\n",
       "        [ 9.67531383e-01,  2.23760414e+00, -5.71979523e-01,\n",
       "          2.96819162e+00],\n",
       "        [ 9.60766137e-01,  2.26864624e+00, -6.76525712e-01,\n",
       "          3.10420489e+00],\n",
       "        [ 9.55962658e-01,  2.30104589e+00, -4.80348855e-01,\n",
       "          3.23996472e+00],\n",
       "        [ 9.51929808e-01,  2.33544159e+00, -4.03283238e-01,\n",
       "          3.43956900e+00],\n",
       "        [ 9.49840903e-01,  2.37213683e+00, -2.08890587e-01,\n",
       "          3.66954112e+00],\n",
       "        [ 9.47403014e-01,  2.40907097e+00, -2.43791878e-01,\n",
       "          3.69340491e+00],\n",
       "        [ 9.45691049e-01,  2.44735837e+00, -1.71195179e-01,\n",
       "          3.82873178e+00],\n",
       "        [ 9.43661332e-01,  2.48771501e+00, -2.02969477e-01,\n",
       "          4.03567457e+00],\n",
       "        [ 9.43590224e-01,  2.52617288e+00, -7.11360062e-03,\n",
       "          3.84577799e+00],\n",
       "        [ 9.42262113e-01,  2.56392431e+00, -1.32808954e-01,\n",
       "          3.77515244e+00],\n",
       "        [ 9.42406237e-01,  2.59992814e+00,  1.44127021e-02,\n",
       "          3.60039091e+00],\n",
       "        [ 9.41730499e-01,  2.63575745e+00, -6.75764903e-02,\n",
       "          3.58292174e+00],\n",
       "        [ 9.41236496e-01,  2.66911960e+00, -4.93982099e-02,\n",
       "          3.33622479e+00],\n",
       "        [ 9.42168772e-01,  2.70002079e+00,  9.32267234e-02,\n",
       "          3.09011531e+00],\n",
       "        [ 9.43508327e-01,  2.72846675e+00,  1.33955240e-01,\n",
       "          2.84459186e+00],\n",
       "        [ 9.44852769e-01,  2.75447607e+00,  1.34442478e-01,\n",
       "          2.60092783e+00],\n",
       "        [ 9.47568655e-01,  2.77823162e+00,  2.71589458e-01,\n",
       "          2.37555051e+00],\n",
       "        [ 9.49631453e-01,  2.79964638e+00,  2.06283242e-01,\n",
       "          2.14149308e+00],\n",
       "        [ 9.52779412e-01,  2.81962490e+00,  3.14793289e-01,\n",
       "          1.99783814e+00],\n",
       "        [ 9.58301485e-01,  2.83849144e+00,  5.52207410e-01,\n",
       "          1.88665223e+00],\n",
       "        [ 9.66192067e-01,  2.85598350e+00,  7.89056122e-01,\n",
       "          1.74920332e+00],\n",
       "        [ 9.74734247e-01,  2.87105203e+00,  8.54220629e-01,\n",
       "          1.50687349e+00],\n",
       "        [ 9.84491348e-01,  2.88405347e+00,  9.75712359e-01,\n",
       "          1.30014038e+00],\n",
       "        [ 9.96421814e-01,  2.89464378e+00,  1.19304633e+00,\n",
       "          1.05901551e+00],\n",
       "        [ 1.01070547e+00,  2.90315747e+00,  1.42836869e+00,\n",
       "          8.51386964e-01],\n",
       "        [ 1.02587545e+00,  2.91001534e+00,  1.51699555e+00,\n",
       "          6.85787857e-01],\n",
       "        [ 1.04339099e+00,  2.91447544e+00,  1.75154650e+00,\n",
       "          4.45990741e-01],\n",
       "        [ 1.06324637e+00,  2.91717553e+00,  1.98553872e+00,\n",
       "          2.70014822e-01],\n",
       "        [ 1.08495390e+00,  2.91941214e+00,  2.17075825e+00,\n",
       "          2.23667517e-01],\n",
       "        [ 1.10871744e+00,  2.92151856e+00,  2.37635732e+00,\n",
       "          2.10640758e-01],\n",
       "        [ 1.13443685e+00,  2.92136121e+00,  2.57193494e+00,\n",
       "         -1.57483108e-02],\n",
       "        [ 1.16247654e+00,  2.92289805e+00,  2.80397344e+00,\n",
       "          1.53694317e-01],\n",
       "        [ 1.19283116e+00,  2.92550516e+00,  3.03545928e+00,\n",
       "          2.60720789e-01],\n",
       "        [ 1.22549510e+00,  2.92681932e+00,  3.26639366e+00,\n",
       "          1.31405503e-01],\n",
       "        [ 1.26046288e+00,  2.92789030e+00,  3.49677801e+00,\n",
       "          1.07089698e-01],\n",
       "        [ 1.29755211e+00,  2.92775941e+00,  3.70892525e+00,\n",
       "         -1.30855106e-02],\n",
       "        [ 1.33684158e+00,  2.92612481e+00,  3.92894387e+00,\n",
       "         -1.63451925e-01],\n",
       "        [ 1.37841904e+00,  2.92434788e+00,  4.15775061e+00,\n",
       "         -1.77702054e-01],\n",
       "        [ 1.42169738e+00,  2.92178607e+00,  4.32782507e+00,\n",
       "         -2.56164908e-01],\n",
       "        [ 1.46725416e+00,  2.91967130e+00,  4.55568171e+00,\n",
       "         -2.11476073e-01],\n",
       "        [ 1.51508415e+00,  2.91726422e+00,  4.78299522e+00,\n",
       "         -2.40719602e-01],\n",
       "        [ 1.56397069e+00,  2.91537595e+00,  4.88865519e+00,\n",
       "         -1.88829646e-01],\n",
       "        [ 1.61223304e+00,  2.91399288e+00,  4.82624054e+00,\n",
       "         -1.38305783e-01],\n",
       "        [ 1.65799892e+00,  2.91299796e+00,  4.57658243e+00,\n",
       "         -9.94831771e-02],\n",
       "        [ 1.70159030e+00,  2.91270733e+00,  4.35914230e+00,\n",
       "         -2.90757306e-02],\n",
       "        [ 1.74355447e+00,  2.91068602e+00,  4.19642019e+00,\n",
       "         -2.02115029e-01],\n",
       "        [ 1.78303707e+00,  2.90854764e+00,  3.94826198e+00,\n",
       "         -2.13849664e-01],\n",
       "        [ 1.82004404e+00,  2.90403247e+00,  3.70069480e+00,\n",
       "         -4.51504201e-01],\n",
       "        [ 1.85561788e+00,  2.90099454e+00,  3.55737662e+00,\n",
       "         -3.03790510e-01],\n",
       "        [ 1.88872528e+00,  2.89858246e+00,  3.31074047e+00,\n",
       "         -2.41213232e-01],\n",
       "        [ 1.92084897e+00,  2.89855766e+00,  3.21237564e+00,\n",
       "         -2.47491361e-03],\n",
       "        [ 1.95051455e+00,  2.89786506e+00,  2.96656108e+00,\n",
       "         -6.92818537e-02],\n",
       "        [ 1.98249125e+00,  2.89955544e+00,  3.19765973e+00,\n",
       "          1.69046998e-01],\n",
       "        [ 2.01673317e+00,  2.90257192e+00,  3.42419815e+00,\n",
       "          3.01650941e-01],\n",
       "        [ 2.05174351e+00,  2.90641785e+00,  3.50103688e+00,\n",
       "          3.84602398e-01],\n",
       "        [ 2.08905220e+00,  2.91162705e+00,  3.73086262e+00,\n",
       "          5.20904541e-01],\n",
       "        [ 2.12815619e+00,  2.91651297e+00,  3.91039014e+00,\n",
       "          4.88588750e-01],\n",
       "        [ 2.16954851e+00,  2.92256832e+00,  4.13924074e+00,\n",
       "          6.05553925e-01],\n",
       "        [ 2.21322393e+00,  2.92972398e+00,  4.36754656e+00,\n",
       "          7.15564668e-01],\n",
       "        [ 2.25917697e+00,  2.93674731e+00,  4.59530830e+00,\n",
       "          7.02313423e-01],\n",
       "        [ 2.30740237e+00,  2.94401407e+00,  4.82252789e+00,\n",
       "          7.26678133e-01],\n",
       "        [ 2.35789442e+00,  2.95179605e+00,  5.04920626e+00,\n",
       "          7.78210640e-01],\n",
       "        [ 2.41015697e+00,  2.95955825e+00,  5.22625542e+00,\n",
       "          7.76216090e-01],\n",
       "        [ 2.46053910e+00,  2.96536112e+00,  5.03822803e+00,\n",
       "          5.80296934e-01],\n",
       "        [ 2.50961065e+00,  2.96951675e+00,  4.90714598e+00,\n",
       "          4.15555686e-01],\n",
       "        [ 2.55934119e+00,  2.97411847e+00,  4.97305870e+00,\n",
       "          4.60173965e-01],\n",
       "        [ 2.60802913e+00,  2.97756147e+00,  4.86877632e+00,\n",
       "          3.44287127e-01],\n",
       "        [ 2.65434575e+00,  2.98137617e+00,  4.63167620e+00,\n",
       "          3.81471217e-01],\n",
       "        [ 2.69827724e+00,  2.98642159e+00,  4.39314938e+00,\n",
       "          5.04547715e-01],\n",
       "        [ 2.74104476e+00,  2.99076962e+00,  4.27673721e+00,\n",
       "          4.34794277e-01],\n",
       "        [ 2.78132844e+00,  2.99534988e+00,  4.02838755e+00,\n",
       "          4.58025128e-01],\n",
       "        [ 2.81945324e+00,  2.99823737e+00,  3.81247330e+00,\n",
       "          2.88748384e-01],\n",
       "        [ 2.85510564e+00,  3.00007701e+00,  3.56522965e+00,\n",
       "          1.83970898e-01],\n",
       "        [ 2.88868594e+00,  3.00028944e+00,  3.35802913e+00,\n",
       "          2.12489776e-02],\n",
       "        [ 2.91980457e+00,  3.00079751e+00,  3.11186767e+00,\n",
       "          5.08047864e-02],\n",
       "        [ 2.91980457e+00,  3.00079751e+00,  3.11186767e+00,\n",
       "          5.08047864e-02],\n",
       "        [ 2.97575378e+00,  2.99969268e+00,  2.67489791e+00,\n",
       "         -1.57398611e-01],\n",
       "        [ 3.00142694e+00,  2.99820709e+00,  2.56731653e+00,\n",
       "         -1.48569450e-01],\n",
       "        [ 3.02465725e+00,  2.99632668e+00,  2.32303834e+00,\n",
       "         -1.88033074e-01],\n",
       "        [ 3.04545379e+00,  2.99634218e+00,  2.07966304e+00,\n",
       "          1.54652749e-03],\n",
       "        [ 3.06381941e+00,  2.99397612e+00,  1.83654618e+00,\n",
       "         -2.36620992e-01],\n",
       "        [ 3.07975936e+00,  2.99051166e+00,  1.59400833e+00,\n",
       "         -3.46440673e-01],\n",
       "        [ 3.09451866e+00,  2.98761606e+00,  1.47592485e+00,\n",
       "         -2.89547712e-01],\n",
       "        [ 3.10686111e+00,  2.98518443e+00,  1.23424590e+00,\n",
       "         -2.43167251e-01],\n",
       "        [ 3.11725545e+00,  2.98447537e+00,  1.03942394e+00,\n",
       "         -7.08981752e-02],\n",
       "        [ 3.12524319e+00,  2.98516226e+00,  7.98784494e-01,\n",
       "          6.86826706e-02],\n",
       "        [ 3.13129926e+00,  2.98493814e+00,  6.05602562e-01,\n",
       "         -2.24052500e-02],\n",
       "        [ 3.13600826e+00,  2.98381352e+00,  4.70916241e-01,\n",
       "         -1.12468712e-01],\n",
       "        [ 3.13865352e+00,  2.98420191e+00,  2.64514834e-01,\n",
       "          3.88384275e-02],\n",
       "        [ 3.14073992e+00,  2.98446012e+00,  2.08633587e-01,\n",
       "          2.58183610e-02],\n",
       "        [ 3.14084601e+00,  2.98566866e+00,  1.06242765e-02,\n",
       "          1.20862812e-01],\n",
       "        [ 3.14082408e+00,  2.98730016e+00, -2.21341988e-03,\n",
       "          1.63146481e-01],\n",
       "        [ 3.13842034e+00,  2.98928022e+00, -2.40371987e-01,\n",
       "          1.97987869e-01],\n",
       "        [ 3.13380885e+00,  2.99095130e+00, -4.61129010e-01,\n",
       "          1.67114273e-01],\n",
       "        [ 3.12933397e+00,  2.99109411e+00, -4.47504997e-01,\n",
       "          1.42936241e-02],\n",
       "        [ 3.12398195e+00,  2.99127102e+00, -5.35186529e-01,\n",
       "          1.76754277e-02],\n",
       "        [ 3.11628819e+00,  2.99304962e+00, -7.69390404e-01,\n",
       "          1.77879781e-01],\n",
       "        [ 3.10643220e+00,  2.99479270e+00, -9.85603988e-01,\n",
       "          1.74297452e-01],\n",
       "        [ 3.09421778e+00,  2.99527669e+00, -1.22142041e+00,\n",
       "          4.83985320e-02],\n",
       "        [ 3.07965112e+00,  2.99456382e+00, -1.45667529e+00,\n",
       "         -7.12886006e-02],\n",
       "        [ 3.06305003e+00,  2.99598145e+00, -1.66011739e+00,\n",
       "          1.41773254e-01],\n",
       "        [ 3.04410672e+00,  2.99819207e+00, -1.89432740e+00,\n",
       "          2.21047089e-01],\n",
       "        [ 3.02282691e+00,  2.99960566e+00, -2.12797976e+00,\n",
       "          1.41362607e-01],\n",
       "        [ 2.99926758e+00,  3.00108719e+00, -2.35593772e+00,\n",
       "          1.48143113e-01],\n",
       "        [ 2.97348976e+00,  3.00129628e+00, -2.57778430e+00,\n",
       "          2.09289901e-02],\n",
       "        [ 2.94539165e+00,  3.00388670e+00, -2.80980873e+00,\n",
       "          2.59042978e-01],\n",
       "        [ 2.91497874e+00,  3.00652862e+00, -3.04128075e+00,\n",
       "          2.64180034e-01],\n",
       "        [ 2.88379884e+00,  3.00855446e+00, -3.11799192e+00,\n",
       "          2.02585191e-01],\n",
       "        [ 2.85031152e+00,  3.00819373e+00, -3.34872985e+00,\n",
       "         -3.60611305e-02],\n",
       "        [ 2.81520128e+00,  3.00820518e+00, -3.51103330e+00,\n",
       "          1.14143139e-03],\n",
       "        [ 2.77779293e+00,  3.00942826e+00, -3.74083519e+00,\n",
       "          1.22299850e-01],\n",
       "        [ 2.73869801e+00,  3.01032543e+00, -3.90948534e+00,\n",
       "          8.97250250e-02],\n",
       "        [ 2.69827962e+00,  3.01061988e+00, -4.04183388e+00,\n",
       "          2.94498503e-02],\n",
       "        [ 2.65595365e+00,  3.01116896e+00, -4.23260117e+00,\n",
       "          5.49004786e-02],\n",
       "        [ 2.61134672e+00,  3.01169086e+00, -4.46068478e+00,\n",
       "          5.21897525e-02],\n",
       "        [ 2.56446457e+00,  3.01109195e+00, -4.68822479e+00,\n",
       "         -5.98970503e-02],\n",
       "        [ 2.51531243e+00,  3.00926900e+00, -4.91522264e+00,\n",
       "         -1.82302192e-01],\n",
       "        [ 2.46389556e+00,  3.00891161e+00, -5.14168024e+00,\n",
       "         -3.57376710e-02],\n",
       "        [ 2.41631460e+00,  3.00845242e+00, -4.75809956e+00,\n",
       "         -4.59009334e-02],\n",
       "        [ 2.37018347e+00,  3.00746775e+00, -4.61310053e+00,\n",
       "         -9.84748006e-02],\n",
       "        [ 2.32647920e+00,  3.00697947e+00, -4.37044716e+00,\n",
       "         -4.88386750e-02],\n",
       "        [ 2.28409815e+00,  3.00612712e+00, -4.23809099e+00,\n",
       "         -8.52323696e-02],\n",
       "        [ 2.24370623e+00,  3.00653815e+00, -4.03918219e+00,\n",
       "          4.11086902e-02],\n",
       "        [ 2.20579243e+00,  3.00689411e+00, -3.79139853e+00,\n",
       "          3.55857648e-02],\n",
       "        [ 2.17035031e+00,  3.00695896e+00, -3.54420495e+00,\n",
       "          6.49715913e-03],\n",
       "        [ 2.13737440e+00,  3.00847936e+00, -3.29760003e+00,\n",
       "          1.52029276e-01],\n",
       "        [ 2.10585189e+00,  3.01216960e+00, -3.15224123e+00,\n",
       "          3.69034439e-01],\n",
       "        [ 2.07678628e+00,  3.01622939e+00, -2.90656972e+00,\n",
       "          4.05979604e-01],\n",
       "        [ 2.07678628e+00,  3.01622939e+00, -2.90656972e+00,\n",
       "          4.05979604e-01],\n",
       "        [ 2.01934218e+00,  3.01762795e+00, -2.93652177e+00,\n",
       "         -2.69922782e-02],\n",
       "        [ 1.98766518e+00,  3.01497698e+00, -3.16769171e+00,\n",
       "         -2.65091836e-01],\n",
       "        [ 1.95515418e+00,  3.00995088e+00, -3.25109601e+00,\n",
       "         -5.02624333e-01],\n",
       "        [ 1.92033899e+00,  3.00331879e+00, -3.48151684e+00,\n",
       "         -6.63195968e-01],\n",
       "        [ 1.88322508e+00,  2.99895382e+00, -3.71138906e+00,\n",
       "         -4.36499387e-01],\n",
       "        [ 1.84381795e+00,  2.99324346e+00, -3.94071364e+00,\n",
       "         -5.71045160e-01],\n",
       "        [ 1.80394459e+00,  2.98734140e+00, -3.98734474e+00,\n",
       "         -5.90211749e-01],\n",
       "        [ 1.76178443e+00,  2.98145223e+00, -4.21601248e+00,\n",
       "         -5.88899195e-01],\n",
       "        [ 1.71734309e+00,  2.97366333e+00, -4.44413519e+00,\n",
       "         -7.78893054e-01],\n",
       "        [ 1.67062593e+00,  2.96705461e+00, -4.67171478e+00,\n",
       "         -6.60865188e-01],\n",
       "        [ 1.62163842e+00,  2.96095753e+00, -4.89875221e+00,\n",
       "         -6.09726489e-01],\n",
       "        [ 1.57307124e+00,  2.95594883e+00, -4.85672188e+00,\n",
       "         -5.00852764e-01],\n",
       "        [ 1.52223802e+00,  2.95120978e+00, -5.08331871e+00,\n",
       "         -4.73914921e-01],\n",
       "        [ 1.47135794e+00,  2.94552350e+00, -5.08800840e+00,\n",
       "         -5.68625808e-01],\n",
       "        [ 1.41909540e+00,  2.94072986e+00, -5.22625542e+00,\n",
       "         -4.79363084e-01],\n",
       "        [ 1.37012732e+00,  2.93570495e+00, -4.89680767e+00,\n",
       "         -5.02496064e-01],\n",
       "        [ 1.32119381e+00,  2.93231750e+00, -4.89334297e+00,\n",
       "         -3.38747352e-01],\n",
       "        [ 1.27475858e+00,  2.92938280e+00, -4.64352512e+00,\n",
       "         -2.93470711e-01],\n",
       "        [ 1.23053408e+00,  2.92541432e+00, -4.42244673e+00,\n",
       "         -3.96843195e-01],\n",
       "        [ 1.18610835e+00,  2.92261004e+00, -4.44258308e+00,\n",
       "         -2.80436099e-01],\n",
       "        [ 1.14404202e+00,  2.92157340e+00, -4.20663309e+00,\n",
       "         -1.03645176e-01],\n",
       "        [ 1.10350215e+00,  2.92112112e+00, -4.05398464e+00,\n",
       "         -4.52456288e-02],\n",
       "        [ 1.06544042e+00,  2.92266798e+00, -3.80616546e+00,\n",
       "          1.54688746e-01],\n",
       "        [ 1.02985108e+00,  2.92375946e+00, -3.55893683e+00,\n",
       "          1.09158702e-01],\n",
       "        [ 9.96728122e-01,  2.92474127e+00, -3.31229687e+00,\n",
       "          9.81832519e-02],\n",
       "        [ 9.64977920e-01,  2.92533970e+00, -3.17502069e+00,\n",
       "          5.98338582e-02],\n",
       "        [ 9.35684979e-01,  2.92593336e+00, -2.92929506e+00,\n",
       "          5.93685471e-02],\n",
       "        [ 9.08550620e-01,  2.92456174e+00, -2.71343803e+00,\n",
       "         -1.37175098e-01],\n",
       "        [ 8.83862495e-01,  2.92158723e+00, -2.46881175e+00,\n",
       "         -2.97431976e-01],\n",
       "        [ 8.59978557e-01,  2.91742349e+00, -2.38839293e+00,\n",
       "         -4.16376799e-01],\n",
       "        [ 8.38253260e-01,  2.91139984e+00, -2.17252922e+00,\n",
       "         -6.02364421e-01],\n",
       "        [ 8.18857372e-01,  2.90435338e+00, -1.93958700e+00,\n",
       "         -7.04644680e-01],\n",
       "        [ 8.01286399e-01,  2.89494205e+00, -1.75709963e+00,\n",
       "         -9.41130280e-01],\n",
       "        [ 7.86138892e-01,  2.88337469e+00, -1.51475108e+00,\n",
       "         -1.15675163e+00],\n",
       "        [ 7.73409069e-01,  2.86945295e+00, -1.27297962e+00,\n",
       "         -1.39216053e+00],\n",
       "        [ 7.63091266e-01,  2.85318303e+00, -1.03178406e+00,\n",
       "         -1.62700868e+00],\n",
       "        [ 7.54875004e-01,  2.83456993e+00, -8.21623206e-01,\n",
       "         -1.86129761e+00],\n",
       "        [ 7.49059975e-01,  2.81414557e+00, -5.81502557e-01,\n",
       "         -2.04243565e+00],\n",
       "        [ 7.45548666e-01,  2.79263377e+00, -3.51134330e-01,\n",
       "         -2.15119386e+00],\n",
       "        [ 7.44427323e-01,  2.76887846e+00, -1.12134203e-01,\n",
       "         -2.37551928e+00],\n",
       "        [ 7.45390356e-01,  2.74332261e+00,  9.63062495e-02,\n",
       "         -2.55558038e+00],\n",
       "        [ 7.48732746e-01,  2.71544600e+00,  3.34240735e-01,\n",
       "         -2.78765774e+00],\n",
       "        [ 7.54448831e-01,  2.68530416e+00,  5.71608543e-01,\n",
       "         -3.01418734e+00],\n",
       "        [ 7.61612654e-01,  2.65285254e+00,  7.16378629e-01,\n",
       "         -3.24517250e+00],\n",
       "        [ 7.71140993e-01,  2.61933637e+00,  9.52836275e-01,\n",
       "         -3.35162210e+00],\n",
       "        [ 7.82578111e-01,  2.58356500e+00,  1.14370811e+00,\n",
       "         -3.57711577e+00],\n",
       "        [ 7.95792520e-01,  2.54585481e+00,  1.32144213e+00,\n",
       "         -3.77103281e+00],\n",
       "        [ 8.11357081e-01,  2.50585270e+00,  1.55645871e+00,\n",
       "         -4.00021553e+00],\n",
       "        [ 8.26538682e-01,  2.46412945e+00,  1.51815939e+00,\n",
       "         -4.17231512e+00],\n",
       "        [ 8.43835115e-01,  2.42012405e+00,  1.72964120e+00,\n",
       "         -4.40054226e+00],\n",
       "        [ 8.63471985e-01,  2.37673616e+00,  1.96368563e+00,\n",
       "         -4.33877945e+00],\n",
       "        [ 8.80680382e-01,  2.33316541e+00,  1.72084510e+00,\n",
       "         -4.35707998e+00],\n",
       "        [ 8.96751523e-01,  2.28909922e+00,  1.60711408e+00,\n",
       "         -4.40661669e+00],\n",
       "        [ 9.10402775e-01,  2.24708176e+00,  1.36512268e+00,\n",
       "         -4.20176315e+00],\n",
       "        [ 9.22473252e-01,  2.20532107e+00,  1.20704746e+00,\n",
       "         -4.17606020e+00],\n",
       "        [ 9.33901072e-01,  2.16590881e+00,  1.14278162e+00,\n",
       "         -3.94121265e+00],\n",
       "        [ 9.44323182e-01,  2.12889719e+00,  1.04221380e+00,\n",
       "         -3.70117497e+00],\n",
       "        [ 9.54626143e-01,  2.09405017e+00,  1.03029621e+00,\n",
       "         -3.48469234e+00],\n",
       "        [ 9.64917064e-01,  2.06127691e+00,  1.02909160e+00,\n",
       "         -3.27732491e+00],\n",
       "        [ 9.72801864e-01,  2.02805567e+00,  7.88476825e-01,\n",
       "         -3.32214594e+00],\n",
       "        [ 9.79688525e-01,  1.99598491e+00,  6.88667297e-01,\n",
       "         -3.20706511e+00],\n",
       "        [ 9.84520316e-01,  1.96577156e+00,  4.83178020e-01,\n",
       "         -3.02133942e+00],\n",
       "        [ 9.90326405e-01,  1.93801177e+00,  5.80608547e-01,\n",
       "         -2.77597976e+00],\n",
       "        [ 9.93736982e-01,  1.90868998e+00,  3.41061890e-01,\n",
       "         -2.93217397e+00],\n",
       "        [ 9.95820343e-01,  1.87905145e+00,  2.08333790e-01,\n",
       "         -2.96385503e+00],\n",
       "        [ 9.97337162e-01,  1.84757972e+00,  1.51681244e-01,\n",
       "         -3.14717412e+00],\n",
       "        [ 9.97829616e-01,  1.81380129e+00,  4.92466688e-02,\n",
       "         -3.37784243e+00],\n",
       "        [ 9.99944627e-01,  1.77772164e+00,  2.11498484e-01,\n",
       "         -3.60796142e+00],\n",
       "        [ 1.00024378e+00,  1.74057770e+00,  2.99175922e-02,\n",
       "         -3.71438837e+00],\n",
       "        [ 1.00030220e+00,  1.70114064e+00,  5.83640020e-03,\n",
       "         -3.94370604e+00],\n",
       "        [ 1.00018811e+00,  1.65941596e+00, -1.13990204e-02,\n",
       "         -4.17247725e+00],\n",
       "        [ 1.00187171e+00,  1.61540890e+00,  1.68352962e-01,\n",
       "         -4.40070391e+00],\n",
       "        [ 1.00136817e+00,  1.57021070e+00, -5.03507927e-02,\n",
       "         -4.51981974e+00],\n",
       "        [ 1.00324750e+00,  1.52279866e+00,  1.87932968e-01,\n",
       "         -4.74119759e+00],\n",
       "        [ 1.00529933e+00,  1.47409356e+00,  2.05183953e-01,\n",
       "         -4.87051439e+00]], dtype=float32),\n",
       " 'actions': array([[-5.68560839e-01, -2.77247220e-01],\n",
       "        [-1.00000000e+00,  6.66650712e-01],\n",
       "        [-1.00000000e+00,  1.00000000e+00],\n",
       "        [-5.75078845e-01,  1.00000000e+00],\n",
       "        [ 1.99982271e-01,  3.38832915e-01],\n",
       "        [-8.56866539e-01,  1.00000000e+00],\n",
       "        [-7.07761168e-01,  1.00000000e+00],\n",
       "        [ 2.81540602e-01,  1.00000000e+00],\n",
       "        [ 3.05802703e-01,  1.00000000e+00],\n",
       "        [-1.73589423e-01,  1.00000000e+00],\n",
       "        [ 5.58468878e-01,  8.44083965e-01],\n",
       "        [ 1.02694035e-01,  6.31068766e-01],\n",
       "        [ 4.03427720e-01,  1.00000000e+00],\n",
       "        [ 1.76731199e-01,  1.00000000e+00],\n",
       "        [ 9.37480390e-01,  3.24740499e-01],\n",
       "        [-6.28444076e-01,  9.11167413e-02],\n",
       "        [-1.09400479e-02,  9.85834777e-01],\n",
       "        [-4.44687426e-01,  6.00772679e-01],\n",
       "        [ 8.16940308e-01,  6.01069510e-01],\n",
       "        [ 3.18778902e-01,  8.70495617e-01],\n",
       "        [ 8.12181115e-01,  1.00000000e+00],\n",
       "        [-1.48632094e-01,  1.36894673e-01],\n",
       "        [ 3.02380383e-01,  6.05143487e-01],\n",
       "        [-1.35125563e-01,  9.07196701e-01],\n",
       "        [ 8.20328057e-01, -7.56979346e-01],\n",
       "        [-5.27839541e-01, -2.58084357e-01],\n",
       "        [ 6.16824746e-01, -6.96035385e-01],\n",
       "        [-3.44111294e-01, -3.73449214e-02],\n",
       "        [ 7.56510273e-02, -1.00000000e+00],\n",
       "        [ 5.98358214e-01, -1.00000000e+00],\n",
       "        [ 1.71942800e-01, -1.00000000e+00],\n",
       "        [ 3.38535872e-03, -9.94648516e-01],\n",
       "        [ 5.77195764e-01, -9.20302927e-01],\n",
       "        [-2.71491230e-01, -9.59002316e-01],\n",
       "        [ 4.57673818e-01, -5.81761897e-01],\n",
       "        [ 1.00000000e+00, -4.46868062e-01],\n",
       "        [ 1.00000000e+00, -5.58252275e-01],\n",
       "        [ 2.81502664e-01, -1.00000000e+00],\n",
       "        [ 5.18660545e-01, -8.52960527e-01],\n",
       "        [ 9.22296762e-01, -9.99431014e-01],\n",
       "        [ 1.00000000e+00, -8.61198843e-01],\n",
       "        [ 3.86409193e-01, -6.86801791e-01],\n",
       "        [ 1.00000000e+00, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -7.34426081e-01],\n",
       "        [ 7.97553420e-01, -1.91902488e-01],\n",
       "        [ 8.84974539e-01, -5.24599068e-02],\n",
       "        [ 8.44953179e-01, -9.48453844e-01],\n",
       "        [ 1.00000000e+00,  7.11296558e-01],\n",
       "        [ 1.00000000e+00,  4.50918734e-01],\n",
       "        [ 1.00000000e+00, -5.40360570e-01],\n",
       "        [ 1.00000000e+00, -1.00782916e-01],\n",
       "        [ 9.25729036e-01, -5.03519595e-01],\n",
       "        [ 9.60901499e-01, -6.31487846e-01],\n",
       "        [ 1.00000000e+00, -6.14678226e-02],\n",
       "        [ 7.55684972e-01, -3.31226051e-01],\n",
       "        [ 1.00000000e+00,  1.85077354e-01],\n",
       "        [ 1.00000000e+00, -1.24902233e-01],\n",
       "        [ 4.91472453e-01,  2.15467840e-01],\n",
       "        [-2.13177830e-01,  2.10250810e-01],\n",
       "        [-1.00000000e+00,  1.61624938e-01],\n",
       "        [-8.67219150e-01,  2.94631273e-01],\n",
       "        [-6.39645994e-01, -7.26846457e-01],\n",
       "        [-1.00000000e+00, -5.12923859e-02],\n",
       "        [-1.00000000e+00, -1.00000000e+00],\n",
       "        [-5.64755321e-01,  6.15703702e-01],\n",
       "        [-1.00000000e+00,  2.59710908e-01],\n",
       "        [-3.79905462e-01,  1.00000000e+00],\n",
       "        [-1.00000000e+00, -2.80533075e-01],\n",
       "        [ 1.00000000e+00,  1.00000000e+00],\n",
       "        [ 9.83163953e-01,  5.58466673e-01],\n",
       "        [ 3.56871933e-01,  3.51312220e-01],\n",
       "        [ 1.00000000e+00,  5.76150119e-01],\n",
       "        [ 7.91106820e-01, -1.30478114e-01],\n",
       "        [ 1.00000000e+00,  4.95998055e-01],\n",
       "        [ 1.00000000e+00,  4.67967629e-01],\n",
       "        [ 1.00000000e+00, -4.84836586e-02],\n",
       "        [ 1.00000000e+00,  1.09325536e-01],\n",
       "        [ 1.00000000e+00,  2.23641098e-01],\n",
       "        [ 1.00000000e+00, -5.92689088e-04],\n",
       "        [ 2.10511357e-01, -8.14861298e-01],\n",
       "        [-3.39874148e-01, -6.85910940e-01],\n",
       "        [ 3.25824410e-01,  1.91498384e-01],\n",
       "        [-3.88129920e-01, -4.81982827e-01],\n",
       "        [-9.46844101e-01,  1.59571052e-01],\n",
       "        [-9.55208719e-01,  5.20587087e-01],\n",
       "        [-4.44858164e-01, -2.87834585e-01],\n",
       "        [-1.00000000e+00,  1.01889484e-01],\n",
       "        [-8.66295338e-01, -7.06177413e-01],\n",
       "        [-1.00000000e+00, -4.37051177e-01],\n",
       "        [-8.34338546e-01, -6.81395471e-01],\n",
       "        [-1.00000000e+00,  1.24311134e-01],\n",
       "        [-7.74425626e-01, -1.57770216e-02],\n",
       "        [-1.00000000e+00, -8.57448041e-01],\n",
       "        [-4.24962550e-01,  3.54977734e-02],\n",
       "        [-1.00000000e+00, -1.67185158e-01],\n",
       "        [-9.98651624e-01,  7.94124603e-01],\n",
       "        [-1.00000000e+00, -1.00000000e+00],\n",
       "        [-1.00000000e+00, -4.63475943e-01],\n",
       "        [-4.79867846e-01,  2.35417172e-01],\n",
       "        [-1.00000000e+00,  1.91846356e-01],\n",
       "        [-8.05674136e-01,  7.20889986e-01],\n",
       "        [-1.00000000e+00,  5.85361719e-01],\n",
       "        [-8.03142548e-01, -3.81772250e-01],\n",
       "        [-5.59463680e-01, -3.78381610e-01],\n",
       "        [-8.61927092e-01,  6.34182215e-01],\n",
       "        [-2.31988430e-01, -5.42801358e-02],\n",
       "        [-8.29313219e-01,  3.99329901e-01],\n",
       "        [-5.37965521e-02,  1.78748906e-01],\n",
       "        [-1.00000000e+00,  1.47923127e-01],\n",
       "        [-9.29316163e-01, -1.27651885e-01],\n",
       "        [ 5.25930673e-02, -6.39990687e-01],\n",
       "        [-3.72631311e-01,  1.43424235e-02],\n",
       "        [-9.88724947e-01,  6.72841191e-01],\n",
       "        [-9.15529191e-01, -1.32626230e-02],\n",
       "        [-1.00000000e+00, -5.26880205e-01],\n",
       "        [-1.00000000e+00, -5.02057195e-01],\n",
       "        [-8.68777215e-01,  8.93889129e-01],\n",
       "        [-1.00000000e+00,  3.34271967e-01],\n",
       "        [-1.00000000e+00, -3.32367927e-01],\n",
       "        [-9.78428185e-01,  2.98835244e-02],\n",
       "        [-9.55046475e-01, -5.32664001e-01],\n",
       "        [-1.00000000e+00,  1.00000000e+00],\n",
       "        [-1.00000000e+00,  2.41598208e-02],\n",
       "        [-3.52506906e-01, -2.55982012e-01],\n",
       "        [-1.00000000e+00, -1.00000000e+00],\n",
       "        [-7.14965940e-01,  1.55845135e-01],\n",
       "        [-1.00000000e+00,  5.08730173e-01],\n",
       "        [-7.45534599e-01, -1.35551840e-01],\n",
       "        [-5.94798267e-01, -2.52185553e-01],\n",
       "        [-8.41411352e-01,  1.07156344e-01],\n",
       "        [-1.00000000e+00, -1.08327651e-02],\n",
       "        [-1.00000000e+00, -4.70107079e-01],\n",
       "        [-1.00000000e+00, -5.14552474e-01],\n",
       "        [-1.00000000e+00,  6.13570631e-01],\n",
       "        [ 9.65689600e-01, -4.30307835e-02],\n",
       "        [ 5.61239421e-01, -2.21205652e-01],\n",
       "        [ 9.72718596e-01,  2.07426935e-01],\n",
       "        [ 5.12031138e-01, -1.53297886e-01],\n",
       "        [ 7.92796075e-01,  5.29627264e-01],\n",
       "        [ 1.00000000e+00, -2.27785185e-02],\n",
       "        [ 1.00000000e+00, -1.21781088e-01],\n",
       "        [ 1.00000000e+00,  6.11123800e-01],\n",
       "        [ 5.77355742e-01,  9.12679434e-01],\n",
       "        [ 1.00000000e+00,  1.58815384e-01],\n",
       "        [ 3.85263503e-01, -1.00000000e+00],\n",
       "        [-5.68169773e-01, -8.12229812e-01],\n",
       "        [-1.00000000e+00, -1.00000000e+00],\n",
       "        [-3.81873906e-01, -1.00000000e+00],\n",
       "        [-1.00000000e+00, -6.79232895e-01],\n",
       "        [-1.00000000e+00,  9.45219398e-01],\n",
       "        [-1.00000000e+00, -5.69294453e-01],\n",
       "        [-2.35201374e-01, -8.61869827e-02],\n",
       "        [-1.00000000e+00, -3.90792615e-04],\n",
       "        [-1.00000000e+00, -8.03633451e-01],\n",
       "        [-1.00000000e+00,  4.87785429e-01],\n",
       "        [-1.00000000e+00,  2.08111942e-01],\n",
       "        [ 1.27489030e-01,  4.51040566e-01],\n",
       "        [-1.00000000e+00,  1.08097918e-01],\n",
       "        [-4.19529706e-01, -4.02410209e-01],\n",
       "        [-1.00000000e+00,  3.69109124e-01],\n",
       "        [ 3.83283883e-01, -1.01924129e-01],\n",
       "        [-3.44214216e-02,  6.82521403e-01],\n",
       "        [ 1.00000000e+00,  1.86719716e-01],\n",
       "        [ 8.81824911e-01, -4.36974138e-01],\n",
       "        [-1.28771082e-01,  4.84800518e-01],\n",
       "        [ 9.46278214e-01,  7.39503682e-01],\n",
       "        [ 5.98872840e-01,  2.44170994e-01],\n",
       "        [ 1.00000000e+00,  8.39030027e-01],\n",
       "        [ 1.00000000e+00, -1.89624220e-01],\n",
       "        [ 1.00000000e+00, -4.49920259e-02],\n",
       "        [ 5.43270350e-01, -1.60039231e-01],\n",
       "        [ 1.00000000e+00, -1.35540916e-03],\n",
       "        [ 8.77045870e-01, -8.24651897e-01],\n",
       "        [ 1.00000000e+00, -6.74256742e-01],\n",
       "        [ 3.12973559e-01, -5.02398610e-01],\n",
       "        [ 8.82482350e-01, -7.85086751e-01],\n",
       "        [ 9.56350386e-01, -4.35476959e-01],\n",
       "        [ 7.46830285e-01, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -9.14760053e-01],\n",
       "        [ 1.00000000e+00, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -1.00000000e+00],\n",
       "        [ 8.72103333e-01, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -7.79173911e-01],\n",
       "        [ 9.61452782e-01, -4.77077276e-01],\n",
       "        [ 1.00000000e+00, -9.63406801e-01],\n",
       "        [ 8.74076426e-01, -7.79794216e-01],\n",
       "        [ 1.00000000e+00, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -9.79027152e-01],\n",
       "        [ 6.13575339e-01, -1.00000000e+00],\n",
       "        [ 1.00000000e+00, -4.79411393e-01],\n",
       "        [ 8.10958803e-01, -9.80316699e-01],\n",
       "        [ 7.57705033e-01, -8.49987507e-01],\n",
       "        [ 1.00000000e+00, -1.00000000e+00],\n",
       "        [-1.45246431e-01, -7.62613773e-01],\n",
       "        [ 9.03149545e-01, -1.00000000e+00],\n",
       "        [ 1.00000000e+00,  2.15322569e-01],\n",
       "        [-1.00000000e+00, -1.20227590e-01],\n",
       "        [-4.60323840e-01, -2.51565218e-01],\n",
       "        [-1.00000000e+00,  8.16072464e-01],\n",
       "        [-6.50073826e-01,  6.59020543e-02],\n",
       "        [-2.57768244e-01,  9.44315732e-01],\n",
       "        [-4.10835415e-01,  9.68455851e-01],\n",
       "        [-3.96173708e-02,  8.71953011e-01],\n",
       "        [ 5.24510723e-03,  8.35846066e-01],\n",
       "        [-1.00000000e+00, -2.20967665e-01],\n",
       "        [-4.11194354e-01,  4.49979156e-01],\n",
       "        [-8.55919778e-01,  7.47752249e-01],\n",
       "        [ 4.13922042e-01,  1.00000000e+00],\n",
       "        [-1.00000000e+00, -6.83585823e-01],\n",
       "        [-5.53886771e-01, -1.62344337e-01],\n",
       "        [-2.35788837e-01, -7.99356639e-01],\n",
       "        [-4.28584486e-01, -1.00000000e+00],\n",
       "        [ 6.81753814e-01, -1.00000000e+00],\n",
       "        [-7.60305107e-01, -4.82944041e-01],\n",
       "        [-1.00812696e-01, -1.00000000e+00],\n",
       "        [-7.23095536e-02, -1.00000000e+00],\n",
       "        [ 7.54626870e-01, -1.00000000e+00],\n",
       "        [-9.16607618e-01, -5.44149041e-01],\n",
       "        [ 1.00000000e+00, -9.74717617e-01],\n",
       "        [ 7.43125826e-02, -5.90387106e-01],\n",
       "        [-8.28343704e-02,  2.23960698e-01]], dtype=float32),\n",
       " 'rewards': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_IRIS(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=100000, trajectory_length=7):\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 10\n",
    "#     gamma = 0.99\n",
    "#     # Assume observations are organized by trajectories\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     for iteration in range(num_iterations):\n",
    "#         # Sample a trajectory index (rather than random individual steps)\n",
    "#         trajectory_idx = np.random.choice(num_trajectories)\n",
    "\n",
    "#         # Define start and end indices for the trajectory\n",
    "#         start_idx = trajectory_idx * trajectory_length\n",
    "#         end_idx = start_idx + trajectory_length\n",
    "\n",
    "#         # Sample the entire trajectory (states, actions, goals, rewards)\n",
    "#         states = torch.tensor(\n",
    "#             dataset['observations'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = torch.tensor(\n",
    "#             dataset['actions'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         goals = torch.tensor(dataset['infos/goal']\n",
    "#                              [start_idx:end_idx], dtype=torch.float32)\n",
    "#         rewards = torch.tensor(\n",
    "#             dataset['rewards'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "#         # Perform the sampling operation\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg+gamma*max_value.detach()\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         if iteration % 1000 == 0:\n",
    "#             print(\n",
    "#                 f\"Iteration {iteration}: VAE Loss: {vae_loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}, Value Loss: {value_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 20\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# train_IRIS(low_level_policy, goal_proposal_vae,\n",
    "#            action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# def train_IRIS(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=20000, trajectory_length=50):\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.001)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 10\n",
    "#     gamma = 0.99\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     # Variables to store cumulative statistics\n",
    "#     cumulative_rewards = []\n",
    "#     cumulative_policy_loss = []\n",
    "#     cumulative_vae_loss = []\n",
    "#     cumulative_value_loss = []\n",
    "\n",
    "#     for iteration in range(num_iterations):\n",
    "#         trajectory_idx = np.random.choice(num_trajectories)\n",
    "#         start_idx = trajectory_idx * trajectory_length\n",
    "#         end_idx = start_idx + trajectory_length\n",
    "\n",
    "#         states = torch.tensor(\n",
    "#             dataset['observations'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         actions = torch.tensor(\n",
    "#             dataset['actions'][start_idx:end_idx], dtype=torch.float32)\n",
    "#         rewards = torch.tensor(\n",
    "#             dataset['rewards'][start_idx:end_idx], dtype=torch.float32)\n",
    "\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "\n",
    "#         # Perform sampling and value update\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg + gamma * max_value.detach()\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "#         # Update optimizers\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         # Store losses and reward\n",
    "#         cumulative_rewards.append(reward_sg.item())\n",
    "#         cumulative_policy_loss.append(policy_loss.item())\n",
    "#         cumulative_vae_loss.append(vae_loss.item())\n",
    "#         cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "#         # Print averages every 1000 iterations\n",
    "#         if iteration % 1000 == 0 and iteration > 0:\n",
    "#             avg_reward = np.mean(cumulative_rewards)\n",
    "#             avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "#             avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "#             avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "#             print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "#                   f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "#                   f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "#                   f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "#             # Reset cumulative statistics\n",
    "#             cumulative_rewards = []\n",
    "#             cumulative_policy_loss = []\n",
    "#             cumulative_vae_loss = []\n",
    "#             cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 20\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# train_IRIS(low_level_policy, goal_proposal_vae,\n",
    "#            action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "Run this training loop for full trajecory from dataset'''\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, trajectories, trajectory_length=5):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "    iteration = 0\n",
    "    for trajectory in trajectories:\n",
    "        # Move data to GPU\n",
    "        iteration += 1\n",
    "        states = torch.tensor(\n",
    "            trajectory['observations'], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            trajectory['actions'], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            trajectory['rewards'], dtype=torch.float32).to(device)\n",
    "        if len(rewards) == 1:\n",
    "            continue\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-2]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 1000 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "\n",
    "def train_IRIS_traj_sample(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=20000, trajectory_length=20):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        trajectory_idx = random.choice([0, 100000-30])\n",
    "        start_idx = trajectory_idx * trajectory_length\n",
    "        end_idx = start_idx + trajectory_length\n",
    "\n",
    "        # Move data to GPU\n",
    "        states = torch.tensor(\n",
    "            dataset['observations'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            dataset['actions'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            dataset['rewards'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "        if len(states) <= 1:\n",
    "            continue\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-2]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 1000 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 20\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_traj_sample(low_level_policy, goal_proposal_vae,\n",
    "                       action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthi/.local/lib/python3.10/site-packages/gym/utils/seeding.py:38: DeprecationWarning: \u001b[33mWARN: Function `rng.randn(*size)` is marked as deprecated and will be removed in the future. Please use `rng.standard_normal(size)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "/tmp/ipykernel_13961/119378754.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;241m/\u001b[39mnum_episodes\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.0\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Visualize the learned policy as a video\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[43mvisualize_policy_as_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow_level_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 40\u001b[0m, in \u001b[0;36mvisualize_policy_as_video\u001b[0;34m(low_level_policy, env, num_episodes, max_steps, save_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)\n\u001b[1;32m     39\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 40\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mwrite(cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR))\n\u001b[1;32m     42\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/mujoco/mujoco_env.py:168\u001b[0m, in \u001b[0;36mMujocoEnv.render\u001b[0;34m(self, mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_viewer(mode)\u001b[38;5;241m.\u001b[39mrender(width, height, camera_id\u001b[38;5;241m=\u001b[39mcamera_id)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# window size used for old mujoco-py:\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_viewer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pixels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# original image is upside-down, so flip it\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_policy_as_video(low_level_policy, env, num_episodes=10, max_steps=10000, save_path=\"env_policy_video.mp4\"):\n",
    "    # Define video writer using OpenCV\n",
    "    height, width, _ = env.render(mode=\"rgb_array\").shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "    video_writer = cv2.VideoWriter(save_path, fourcc, 30, (width, height))\n",
    "    accuracy = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = state.copy()  # Assuming the goal is part of the observation for simplicity\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Render the environment and capture the frame\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            # Write frame to video\n",
    "            video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "            # VAE update\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "            z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "            goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "\n",
    "            next_state = None\n",
    "            # Get the action from the low-level policy\n",
    "            reward = 0\n",
    "            for _ in range(3):\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action = low_level_policy(\n",
    "                    state_tensor, goal).detach().numpy()\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                action = np.squeeze(action)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    print(\n",
    "                        f\"Episode {episode+1}, Step {step+1}: Goal reached!\")\n",
    "                    break\n",
    "                if done:\n",
    "                    break  # Terminate the episode if done is True\n",
    "            if done:\n",
    "                break\n",
    "            if reward > 0:\n",
    "                accuracy += 1\n",
    "                break\n",
    "    # Release the video writer after finishing\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {save_path}\")\n",
    "    print(f\"Accuracy is {accuracy/num_episodes*100.0:.4f}\")\n",
    "\n",
    "\n",
    "# Visualize the learned policy as a video\n",
    "visualize_policy_as_video(low_level_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(low_level_policy.state_dict(),\n",
    "           \"/home/keerthi/IRIS/low_level_policy_full_traj.pth\")\n",
    "torch.save(goal_proposal_vae.state_dict(),\n",
    "           \"/home/keerthi/IRIS/goal_proposal_cvae_full_traj.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
