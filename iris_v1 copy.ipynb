{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset: http://rail.eecs.berkeley.edu/datasets/offline_rl/maze2d/maze2d-medium-sparse-v1.hdf5 to /home/keerthi/.d4rl/datasets/maze2d-medium-sparse-v1.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load datafile: 100%|██████████| 8/8 [00:00<00:00, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import d4rl\n",
    "env = gym.make(\"maze2d-medium-v1\")\n",
    "dataset = env.get_dataset()\n",
    "print(dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_into_trajectories(dataset):\n",
    "    observations = dataset['observations']\n",
    "    actions = dataset['actions']\n",
    "    rewards = dataset['rewards']\n",
    "    # 'dones' in some datasets are called 'terminals'\n",
    "    dones = dataset['terminals']\n",
    "\n",
    "    trajectories = []\n",
    "    current_trajectory = {\n",
    "        'observations': [],\n",
    "        'actions': [],\n",
    "        'rewards': []\n",
    "    }\n",
    "\n",
    "    for i in range(len(observations)):\n",
    "        # Append the current timestep's data to the current trajectory\n",
    "        current_trajectory['observations'].append(observations[i])\n",
    "        current_trajectory['actions'].append(actions[i])\n",
    "        current_trajectory['rewards'].append(rewards[i])\n",
    "\n",
    "        # If the 'done' flag is True, the current trajectory ends\n",
    "        if rewards[i] == 1:\n",
    "            # Convert lists to numpy arrays\n",
    "            current_trajectory['observations'] = np.array(\n",
    "                current_trajectory['observations'])\n",
    "            current_trajectory['actions'] = np.array(\n",
    "                current_trajectory['actions'])\n",
    "            current_trajectory['rewards'] = np.array(\n",
    "                current_trajectory['rewards'])\n",
    "\n",
    "            # Add the current trajectory to the list of trajectories\n",
    "            trajectories.append(current_trajectory)\n",
    "\n",
    "            # Reset the current trajectory\n",
    "            current_trajectory = {\n",
    "                'observations': [],\n",
    "                'actions': [],\n",
    "                'rewards': []\n",
    "            }\n",
    "\n",
    "    return trajectories\n",
    "\n",
    "\n",
    "# Split the dataset into a list of trajectories\n",
    "trajectories = split_into_trajectories(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47333"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47333 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47333/47333 [24:54<00:00, 31.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "Run this training loop for full trajecory from dataset'''\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, trajectories, trajectory_length=5):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "    iteration = 0\n",
    "    for trajectory in tqdm(trajectories):\n",
    "        # Move data to GPU\n",
    "        iteration += 1\n",
    "        states = torch.tensor(\n",
    "            trajectory['observations'], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            trajectory['actions'], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            trajectory['rewards'], dtype=torch.float32).to(device)\n",
    "        if len(rewards) == 1:\n",
    "            continue\n",
    "        if len(actions) <= 1:\n",
    "            continue\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-2]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 1000 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 8\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# import matplotlib.pyplot as plt  # Importing for plotting\n",
    "\n",
    "# '''\n",
    "# Run this training loop for full trajectory from dataset\n",
    "# '''\n",
    "\n",
    "\n",
    "# def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, trajectories, trajectory_length=5):\n",
    "#     # Move models to GPU\n",
    "#     max_steps = 200\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     low_level_policy = low_level_policy.to(device)\n",
    "#     goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "#     action_vae = action_vae.to(device)\n",
    "#     value_network = value_network.to(device)\n",
    "\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 30\n",
    "#     gamma = 0.99\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     # Variables to store cumulative statistics\n",
    "#     cumulative_rewards = []\n",
    "#     cumulative_policy_loss = []\n",
    "#     cumulative_vae_loss = []\n",
    "#     cumulative_value_loss = []\n",
    "#     iteration = 0\n",
    "\n",
    "#     # For plotting\n",
    "#     rewards_per_iteration = []  # Store average rewards over iterations\n",
    "#     validation_reward = []\n",
    "#     for trajectory in trajectories:\n",
    "#         # Move data to GPU\n",
    "#         iteration += 1\n",
    "#         states = torch.tensor(\n",
    "#             trajectory['observations'], dtype=torch.float32).to(device)\n",
    "#         actions = torch.tensor(\n",
    "#             trajectory['actions'], dtype=torch.float32).to(device)\n",
    "#         rewards = torch.tensor(\n",
    "#             trajectory['rewards'], dtype=torch.float32).to(device)\n",
    "#         if len(rewards) == 1:\n",
    "#             continue\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "\n",
    "#         # Perform sampling and value update\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg + gamma * max_value.detach()\n",
    "#         Vbar = Vbar.unsqueeze(0)\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "#         # Update optimizers\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         # Store losses and reward\n",
    "#         cumulative_rewards.append(reward_sg.item())\n",
    "#         cumulative_policy_loss.append(policy_loss.item())\n",
    "#         cumulative_vae_loss.append(vae_loss.item())\n",
    "#         cumulative_value_loss.append(value_loss.item())\n",
    "#         state = env.reset()\n",
    "#         goal = state.copy()  # Assuming the goal is part of the observation for simplicity\n",
    "\n",
    "#         for step in range(max_steps):\n",
    "\n",
    "#             goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "#             state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "#             mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "#             z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#             goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "\n",
    "#             next_state = None\n",
    "#             # Get the action from the low-level policy\n",
    "#             reward = 0\n",
    "#             for _ in range(3):\n",
    "#                 state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "#                 action = low_level_policy(\n",
    "#                     state_tensor, goal).detach().numpy()\n",
    "#                 frame = env.render(mode=\"rgb_array\")\n",
    "#                 action = np.squeeze(action)\n",
    "#                 next_state, reward, done, _ = env.step(action)\n",
    "#                 frame = env.render(mode=\"rgb_array\")\n",
    "#                 state = next_state\n",
    "#                 if reward > 0:\n",
    "#                     break\n",
    "#                 if done:\n",
    "#                     break  # Terminate the episode if done is True\n",
    "#             if done:\n",
    "#                 validation_reward.append([iter,0])\n",
    "#                 break\n",
    "#             if reward > 0:\n",
    "#                 accuracy += 1\n",
    "#                 validation_reward.append([iter,reward])\n",
    "#                 break\n",
    "#         # Print averages every 1000 iterations\n",
    "#         if iteration % 1000 == 0 and iteration > 0:\n",
    "#             avg_reward = np.mean(cumulative_rewards)\n",
    "#             avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "#             avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "#             avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "#             print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "#                   f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "#                   f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "#                   f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "#             # Reset cumulative statistics\n",
    "#             cumulative_rewards = []\n",
    "#             cumulative_policy_loss = []\n",
    "#             cumulative_vae_loss = []\n",
    "#             cumulative_value_loss = []\n",
    "\n",
    "#         # Store average reward after each iteration for plotting\n",
    "#         rewards_per_iteration.append(np.mean(cumulative_rewards))\n",
    "\n",
    "#     return rewards_per_iteration\n",
    "\n",
    "\n",
    "# def plot_rewards(rewards_per_iteration):\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(rewards_per_iteration, label=\"Mean Reward per Iteration\")\n",
    "#     plt.xlabel(\"Iteration\")\n",
    "#     plt.ylabel(\"Mean Reward\")\n",
    "#     plt.title(\"Mean Reward During Training\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Assuming dataset and models are already initialized\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 8\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# rewards_per_iteration = train_IRIS_full_trajectory(\n",
    "#     low_level_policy, goal_proposal_vae, action_vae, value_network, trajectories)\n",
    "\n",
    "# # Plot the mean reward during training\n",
    "# plot_rewards(rewards_per_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# import random\n",
    "\n",
    "\n",
    "# def train_IRIS_traj_sample(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, dataset, num_iterations=20000, trajectory_length=20):\n",
    "#     # Move models to GPU\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     low_level_policy = low_level_policy.to(device)\n",
    "#     goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "#     action_vae = action_vae.to(device)\n",
    "#     value_network = value_network.to(device)\n",
    "\n",
    "#     policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "#     vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "#     value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "#     action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "#     M = 30\n",
    "#     gamma = 0.99\n",
    "#     num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "#     # Variables to store cumulative statistics\n",
    "#     cumulative_rewards = []\n",
    "#     cumulative_policy_loss = []\n",
    "#     cumulative_vae_loss = []\n",
    "#     cumulative_value_loss = []\n",
    "\n",
    "#     for iteration in range(num_iterations):\n",
    "#         trajectory_idx = random.choice([0, 100000-30])\n",
    "#         start_idx = trajectory_idx * trajectory_length\n",
    "#         end_idx = start_idx + trajectory_length\n",
    "\n",
    "#         # Move data to GPU\n",
    "#         states = torch.tensor(\n",
    "#             dataset['observations'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "#         actions = torch.tensor(\n",
    "#             dataset['actions'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "#         rewards = torch.tensor(\n",
    "#             dataset['rewards'][start_idx:end_idx], dtype=torch.float32).to(device)\n",
    "#         if len(states) <= 1:\n",
    "#             continue\n",
    "#         actions = actions[:-1]\n",
    "#         sg = states[-1]\n",
    "#         s_start = states[0]\n",
    "#         reward_sg = rewards[-2]\n",
    "#         actionlast = actions[-2]\n",
    "#         statesecondlast = states[-2]\n",
    "\n",
    "#         # Train Low-Level Policy\n",
    "#         policy_actions = []\n",
    "#         for state in states:\n",
    "#             policy_actions.append(low_level_policy(state, sg))\n",
    "#         policy_actions = policy_actions[:-1]\n",
    "#         policy_actions = torch.stack(policy_actions)\n",
    "#         policy_actions = torch.squeeze(policy_actions)\n",
    "#         policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "#         policy_optimizer.zero_grad()\n",
    "#         policy_loss.backward()\n",
    "#         policy_optimizer.step()\n",
    "\n",
    "#         # VAE update\n",
    "#         mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "#         z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "#         vae_loss = cvaeLoss(\n",
    "#             sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "#         mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "#         za = action_vae.reparameterize(mua, logvara)\n",
    "#         actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "#             za, statesecondlast), mua, logvara)\n",
    "#         action_optimizer.zero_grad()\n",
    "#         actionvae_loss.backward()\n",
    "#         action_optimizer.step()\n",
    "\n",
    "#         # Perform sampling and value update\n",
    "#         sampled_actions = []\n",
    "#         for _ in range(M):\n",
    "#             sampled_action = action_vae.decode(za, sg)\n",
    "#             sampled_actions.append(sampled_action)\n",
    "#         sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "#         values = []\n",
    "#         for action in sampled_actions:\n",
    "#             value = value_network(sg, action)\n",
    "#             values.append(value)\n",
    "#         values = torch.stack(values)\n",
    "#         max_value = torch.max(values)\n",
    "#         Vbar = reward_sg + gamma * max_value.detach()\n",
    "#         Vbar = Vbar.unsqueeze(0)\n",
    "#         value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "#         # Update optimizers\n",
    "#         vae_optimizer.zero_grad()\n",
    "#         vae_loss.backward()\n",
    "#         vae_optimizer.step()\n",
    "\n",
    "#         value_optimizer.zero_grad()\n",
    "#         value_loss.backward()\n",
    "#         value_optimizer.step()\n",
    "\n",
    "#         # Store losses and reward\n",
    "#         cumulative_rewards.append(reward_sg.item())\n",
    "#         cumulative_policy_loss.append(policy_loss.item())\n",
    "#         cumulative_vae_loss.append(vae_loss.item())\n",
    "#         cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "#         # Print averages every 1000 iterations\n",
    "#         if iteration % 1000 == 0 and iteration > 0:\n",
    "#             avg_reward = np.mean(cumulative_rewards)\n",
    "#             avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "#             avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "#             avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "#             print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "#                   f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "#                   f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "#                   f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "#             # Reset cumulative statistics\n",
    "#             cumulative_rewards = []\n",
    "#             cumulative_policy_loss = []\n",
    "#             cumulative_vae_loss = []\n",
    "#             cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# # Assuming dataset and models are already initialized\n",
    "# state_dim = dataset['observations'].shape[1]\n",
    "# state_goal_dim = dataset['observations'].shape[1]\n",
    "# action_dim = dataset['actions'].shape[1]\n",
    "# latent_dim = 20\n",
    "\n",
    "# low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "# goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "# action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "# value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# # Train the IRIS algorithm using the D4RL dataset\n",
    "# train_IRIS_traj_sample(low_level_policy, goal_proposal_vae,\n",
    "#                        action_vae, value_network, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18837/3690327565.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2, Step 196: Goal reached!\n",
      "Episode 3, Step 110: Goal reached!\n",
      "Episode 4, Step 106: Goal reached!\n",
      "Episode 5, Step 47: Goal reached!\n",
      "Episode 6, Step 18: Goal reached!\n",
      "Episode 7, Step 167: Goal reached!\n",
      "Episode 8, Step 135: Goal reached!\n",
      "Episode 9, Step 183: Goal reached!\n",
      "Episode 10, Step 235: Goal reached!\n",
      "Episode 11, Step 53: Goal reached!\n",
      "Episode 12, Step 19: Goal reached!\n",
      "Episode 13, Step 55: Goal reached!\n",
      "Episode 14, Step 69: Goal reached!\n",
      "Episode 15, Step 107: Goal reached!\n",
      "Episode 16, Step 159: Goal reached!\n",
      "Episode 17, Step 167: Goal reached!\n",
      "Episode 18, Step 1: Goal reached!\n",
      "Episode 20, Step 204: Goal reached!\n",
      "Video saved to env_policy_video_5.mp4\n",
      "Accuracy is 90.0000\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def visualize_policy_as_video(low_level_policy, goal_proposal_vae, value_function, env, num_episodes=20, max_steps=10000, save_path=\"env_policy_video_5.mp4\"):\n",
    "    # Define video writer using OpenCV\n",
    "    height, width, _ = env.render(mode=\"rgb_array\").shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "    video_writer = cv2.VideoWriter(save_path, fourcc, 30, (width, height))\n",
    "    accuracy = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        goal = state.copy()  # Assuming the goal is part of the observation for simplicity\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Render the environment and capture the frame\n",
    "            frame = env.render(mode=\"rgb_array\")\n",
    "            # Write frame to video\n",
    "            video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "            goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "            # VAE update\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "            mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "            z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "            goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "            goal_final = goal\n",
    "            value = value_function(goal, torch.squeeze(\n",
    "                low_level_policy(state_tensor, goal)))\n",
    "            for k in range(5):\n",
    "                z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "                goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "                if value_function(goal, torch.squeeze(low_level_policy(state_tensor, goal))) > value:\n",
    "                    goal_final = goal\n",
    "                    value = value_function(\n",
    "                        goal, torch.squeeze(low_level_policy(state_tensor, goal)))\n",
    "            goal = goal_final\n",
    "            next_state = None\n",
    "            # Get the action from the low-level policy\n",
    "            reward = 0\n",
    "            for _ in range(2):\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "                action = low_level_policy(\n",
    "                    state_tensor, goal).detach().numpy()\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                action = np.squeeze(action)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                frame = env.render(mode=\"rgb_array\")\n",
    "                video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "                state = next_state\n",
    "                if reward > 0:\n",
    "                    print(\n",
    "                        f\"Episode {episode+1}, Step {step+1}: Goal reached!\")\n",
    "                    break\n",
    "                if done:\n",
    "                    break  # Terminate the episode if done is True\n",
    "            if done:\n",
    "                break\n",
    "            if reward > 0:\n",
    "                accuracy += 1\n",
    "                break\n",
    "    # Release the video writer after finishing\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {save_path}\")\n",
    "    print(f\"Accuracy is {accuracy/num_episodes*100.0:.4f}\")\n",
    "\n",
    "\n",
    "# Visualize the learned policy as a video\n",
    "visualize_policy_as_video(low_level_policy.to(\n",
    "    \"cpu\"), goal_proposal_vae.to(\"cpu\"), value_network.to(\"cpu\"), env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(low_level_policy.state_dict(),\n",
    "           \"/home/keerthi/IRIS/low_level_policy_full_traj.pth\")\n",
    "torch.save(goal_proposal_vae.state_dict(),\n",
    "           \"/home/keerthi/IRIS/goal_proposal_cvae_full_traj.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
