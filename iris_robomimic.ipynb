{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=512):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim,\n",
    "                           num_layers=4, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ImprovedLowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=512):\n",
    "        super(ImprovedLowLevelPolicy, self).__init__()\n",
    "\n",
    "        # Bidirectional LSTM with layer normalization\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim,\n",
    "                           num_layers=4, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim * 2),  # for bidirectional LSTM\n",
    "            nn.Linear(hidden_dim * 2, action_dim),\n",
    "            nn.Tanh()  # Tanh to keep action outputs bounded between -1 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        out, _ = self.rnn(input_seq.unsqueeze(0))\n",
    "\n",
    "        # Pass through the fully connected layer with tanh activation\n",
    "        actions = self.fc(out.squeeze(0))\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=32):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(1024, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(1024, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(256)\n",
    "        self.norm2 = nn.LayerNorm(256)\n",
    "        self.norm3 = nn.LayerNorm(128)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat((state, action), dim=-1)\n",
    "\n",
    "        # First layer with normalization and activation\n",
    "        x = nn.functional.relu(self.norm1(self.fc1(x)))\n",
    "\n",
    "        # Second layer with dropout and normalization\n",
    "        x = self.dropout(nn.functional.relu(self.norm2(self.fc2(x))))\n",
    "\n",
    "        # Third layer with normalization and activation\n",
    "        x = nn.functional.relu(self.norm3(self.fc3(x)))\n",
    "\n",
    "        # Fourth layer without normalization\n",
    "        x = nn.functional.relu(self.fc4(x))\n",
    "\n",
    "        # Output layer\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=32):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(1024, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(1024, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mROBOMIMIC WARNING(\n",
      "    No private macro file found!\n",
      "    It is recommended to use a private macro file\n",
      "    To setup, run: python /home/keerthi/IRIS/robomimic/scripts/setup_macros.py\n",
      ")\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.test_utils as TestUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.train_utils as TrainUtils\n",
    "from robomimic.utils.dataset import SequenceDataset\n",
    "\n",
    "\n",
    "def get_data_loader(dataset_path, trajectory_length=8, batch_size=1):\n",
    "    \"\"\"\n",
    "    Get a data loader to sample batches of data.\n",
    "    Args:\n",
    "        dataset_path (str): path to the dataset hdf5\n",
    "        trajectory_length (int): number of timesteps in a trajectory\n",
    "        batch_size (int): batch size for the DataLoader\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    {'cube_pos',\n",
    " 'cube_quat',\n",
    " 'gripper_to_cube_pos',\n",
    " 'robot0_eef_pos',\n",
    " 'robot0_eef_quat',\n",
    " 'robot0_gripper_qpos',\n",
    " 'robot0_gripper_qvel',\n",
    " 'robot0_joint_pos_cos',\n",
    " 'robot0_joint_pos_sin',\n",
    " 'robot0_joint_vel'}\n",
    " \"\"\"\n",
    "    dataset = SequenceDataset(\n",
    "        hdf5_path=dataset_path,\n",
    "        obs_keys=(\"object\",                # observations we want to appear in batches\n",
    "                  \"robot0_eef_pos\",\n",
    "                  \"robot0_eef_quat\",\n",
    "                  \"robot0_gripper_qpos\",\n",
    "                  \"robot0_gripper_qvel\",       # Additional gripper velocity\n",
    "                  # \"robot0_joint_pos\",          # 7DOF joint positions\n",
    "                  \"robot0_joint_pos_cos\",\n",
    "                  \"robot0_joint_pos_sin\",\n",
    "                  \"robot0_joint_vel\",          # 7DOF joint velocities\n",
    "                  ),\n",
    "        dataset_keys=(                  # can optionally specify more keys here if they should appear in batches\n",
    "            \"actions\",\n",
    "            \"rewards\",\n",
    "            \"dones\",\n",
    "            \"horizon\",                  # Additional metadata\n",
    "            \"episode_id\",               # Episode information\n",
    "        ),\n",
    "        load_next_obs=True,             # load the next observation in each sequence\n",
    "        frame_stack=1,\n",
    "        seq_length=trajectory_length,   # length-10 temporal sequences\n",
    "        # pad last obs per trajectory to ensure all sequences are sampled\n",
    "        pad_frame_stack=True,\n",
    "        pad_seq_length=True,            # pad last observation sequence\n",
    "        get_pad_mask=False,             # do not return padding masks\n",
    "        goal_mode=None,\n",
    "        hdf5_cache_mode=\"all\",          # cache dataset in memory to avoid repeated file i/o\n",
    "        hdf5_use_swmr=True,\n",
    "        hdf5_normalize_obs=False,       # normalize observations\n",
    "        filter_by_attribute=None,       # can optionally provide a filter key here\n",
    "    )\n",
    "\n",
    "    print(\"\\n============= Created Dataset =============\")\n",
    "    print(dataset)\n",
    "    # Print additional dataset information\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Observation keys: {dataset.obs_keys}\")\n",
    "    print(f\"Dataset keys: {dataset.dataset_keys}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Set batch size to a manageable number\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        # no custom sampling logic (uniform sampling)\n",
    "        sampler=None,\n",
    "        batch_size=batch_size,          # batches of size 32\n",
    "        shuffle=False,                   # shuffle the dataset\n",
    "        # use 0 workers (could be set to more if using multiprocessing)\n",
    "        num_workers=0,\n",
    "        drop_last=True                  # drop the last incomplete batch\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceDataset: loading dataset into memory...\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 364.56it/s]\n",
      "SequenceDataset: caching get_item calls...\n",
      "100%|██████████| 9666/9666 [00:01<00:00, 8056.65it/s]\n",
      "\n",
      "============= Created Dataset =============\n",
      "SequenceDataset (\n",
      "\tpath=/home/keerthi/IRIS/lift/ph/low_dim_v141.hdf5\n",
      "\tobs_keys=('object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_gripper_qvel', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', 'robot0_joint_vel')\n",
      "\tseq_length=8\n",
      "\tfilter_key=none\n",
      "\tframe_stack=1\n",
      "\tpad_seq_length=True\n",
      "\tpad_frame_stack=True\n",
      "\tgoal_mode=none\n",
      "\tcache_mode=all\n",
      "\tnum_demos=200\n",
      "\tnum_sequences=9666\n",
      ")\n",
      "Dataset length: 9666\n",
      "Observation keys: ('object', 'robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'robot0_gripper_qvel', 'robot0_joint_pos_cos', 'robot0_joint_pos_sin', 'robot0_joint_vel')\n",
      "Dataset keys: ('actions', 'rewards', 'dones', 'horizon', 'episode_id')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_data_loader(\n",
    "    dataset_path=\"/home/keerthi/IRIS/lift/ph/low_dim_v141.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State configuration at batch 0, timestep 6 where reward is 1:\n",
      "tensor([ 0.0298,  0.0221,  0.8355,  0.0237,  0.0270,  0.8314, -0.0080, -0.0045,\n",
      "         0.9694,  0.2452,  0.0061, -0.0049,  0.0041,  0.9862, -0.1468,  0.0732,\n",
      "         0.0202], device='cuda:0', dtype=torch.float64)\n",
      "Goal state configuration found: tensor([ 0.0298,  0.0221,  0.8355,  0.0237,  0.0270,  0.8314, -0.0080, -0.0045,\n",
      "         0.9694,  0.2452,  0.0061, -0.0049,  0.0041,  0.9862, -0.1468,  0.0732,\n",
      "         0.0202], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "goal_state_config = None\n",
    "\n",
    "# Iterate over the DataLoader to get batches of trajectories\n",
    "c = 0\n",
    "for batch in data_loader:\n",
    "    batch_size = batch['obs']['robot0_eef_pos'].shape[0]\n",
    "\n",
    "    # Iterate over each trajectory in the batch\n",
    "    for i in range(batch_size):\n",
    "        rewards = batch['rewards'][i]  # Rewards for the i-th trajectory\n",
    "\n",
    "        # Iterate over timesteps within the trajectory\n",
    "        for j in range(rewards.shape[0]):\n",
    "            if rewards[j] == 1:  # Check if the reward at timestep `j` is 1\n",
    "                # Extract the state configuration at the timestep `j`\n",
    "                c += 1\n",
    "                if c == 1:\n",
    "                    continue\n",
    "                state_config = torch.cat([\n",
    "                    # End-effector position (3,)\n",
    "                    batch['obs']['robot0_eef_pos'][i, j],\n",
    "                    # Object-related observation (10,)\n",
    "                    batch['obs']['object'][i, j],\n",
    "                    # End-effector orientation (4,)\n",
    "                    batch['obs']['robot0_eef_quat'][i, j],\n",
    "                    # Gripper position (2,)\n",
    "                    # batch['obs']['robot0_gripper_qpos'][i, j],\n",
    "                    # Gripper velocity (2,)\n",
    "                    # batch['obs']['robot0_gripper_qvel'][i, j],\n",
    "                    # 7DOF joint positions (7,)\n",
    "                    # batch['obs']['robot0_joint_pos_cos'][i, j],\n",
    "                    # batch['obs']['robot0_joint_pos_sin'][i, j],\n",
    "                    # 7DOF joint velocities (7,)\n",
    "                    # batch['obs']['robot0_joint_vel'][i, j]\n",
    "                ], axis=0).to(device)  # Concatenate along the feature axis\n",
    "\n",
    "                # Print or save the state configuration when goal is attained\n",
    "                print(\n",
    "                    f\"State configuration at batch {i}, timestep {j} where reward is 1:\")\n",
    "                print(state_config)\n",
    "                goal_state_config = state_config  # Save the found state configuration\n",
    "\n",
    "                # Break out of the inner loop once the goal state is found\n",
    "                break\n",
    "        if goal_state_config is not None:\n",
    "            # Break out of the outer loop if a goal state is found\n",
    "            break\n",
    "    if goal_state_config is not None:\n",
    "        # Exit the loop once a goal is found in the batch\n",
    "        break\n",
    "\n",
    "# Print the final goal state configuration if found\n",
    "if goal_state_config is not None:\n",
    "    print(\"Goal state configuration found:\", goal_state_config)\n",
    "else:\n",
    "    print(\"No goal state configuration with a reward of 1 found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_state_config.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0237, 0.0270, 0.8314], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_pos = goal_state_config[3:6]\n",
    "cube_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0080, -0.0045,  0.9694,  0.2452], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_orient = goal_state_config[6:10]\n",
    "cube_orient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 17\n",
    "state_goal_dim = 17\n",
    "action_dim = 7\n",
    "latent_dim = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_level_policy = LowLevelPolicy(\n",
    "    state_dim, state_goal_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataset and models are already initialized\n",
    "\n",
    "\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_network = ValueNetwork(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 104/9666 [00:02<04:06, 38.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100: Avg Reward: 0.1100, Avg Policy Loss: 0.0963, Avg VAE Loss: 0.1495, Avg Value Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 205/9666 [00:05<04:09, 37.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200: Avg Reward: 0.2200, Avg Policy Loss: 0.0952, Avg VAE Loss: 0.1568, Avg Value Loss: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 303/9666 [00:08<04:47, 32.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300: Avg Reward: 0.2200, Avg Policy Loss: 0.0849, Avg VAE Loss: 0.1612, Avg Value Loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 407/9666 [00:10<04:02, 38.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400: Avg Reward: 0.2200, Avg Policy Loss: 0.0464, Avg VAE Loss: 0.1490, Avg Value Loss: 0.0381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 507/9666 [00:13<04:01, 37.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500: Avg Reward: 0.2300, Avg Policy Loss: 0.0831, Avg VAE Loss: 0.1716, Avg Value Loss: 0.0335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 607/9666 [00:16<04:01, 37.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 600: Avg Reward: 0.2200, Avg Policy Loss: 0.0475, Avg VAE Loss: 0.1437, Avg Value Loss: 0.0445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 707/9666 [00:18<03:49, 39.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 700: Avg Reward: 0.2200, Avg Policy Loss: 0.0320, Avg VAE Loss: 0.1406, Avg Value Loss: 0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 807/9666 [00:21<03:45, 39.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 800: Avg Reward: 0.2200, Avg Policy Loss: 0.0693, Avg VAE Loss: 0.1588, Avg Value Loss: 0.0269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 907/9666 [00:23<03:51, 37.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 900: Avg Reward: 0.2200, Avg Policy Loss: 0.0827, Avg VAE Loss: 0.1653, Avg Value Loss: 0.0332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1004/9666 [00:26<03:42, 38.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000: Avg Reward: 0.2200, Avg Policy Loss: 0.0715, Avg VAE Loss: 0.1435, Avg Value Loss: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1105/9666 [00:28<03:39, 39.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1100: Avg Reward: 0.2200, Avg Policy Loss: 0.0471, Avg VAE Loss: 0.1524, Avg Value Loss: 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1205/9666 [00:31<03:36, 39.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1200: Avg Reward: 0.2200, Avg Policy Loss: 0.0575, Avg VAE Loss: 0.1640, Avg Value Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 1307/9666 [00:34<03:33, 39.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1300: Avg Reward: 0.2200, Avg Policy Loss: 0.0266, Avg VAE Loss: 0.1240, Avg Value Loss: 0.0214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1407/9666 [00:36<03:41, 37.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1400: Avg Reward: 0.2200, Avg Policy Loss: 0.0724, Avg VAE Loss: 0.1640, Avg Value Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1505/9666 [00:39<03:31, 38.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1500: Avg Reward: 0.2200, Avg Policy Loss: 0.0353, Avg VAE Loss: 0.1479, Avg Value Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1605/9666 [00:42<03:34, 37.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1600: Avg Reward: 0.2500, Avg Policy Loss: 0.0482, Avg VAE Loss: 0.1323, Avg Value Loss: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1705/9666 [00:44<03:31, 37.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1700: Avg Reward: 0.1900, Avg Policy Loss: 0.0692, Avg VAE Loss: 0.1267, Avg Value Loss: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 1806/9666 [00:47<03:36, 36.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1800: Avg Reward: 0.2200, Avg Policy Loss: 0.0556, Avg VAE Loss: 0.1263, Avg Value Loss: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1906/9666 [00:50<03:20, 38.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1900: Avg Reward: 0.2200, Avg Policy Loss: 0.0946, Avg VAE Loss: 0.1686, Avg Value Loss: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2006/9666 [00:52<03:22, 37.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2000: Avg Reward: 0.2200, Avg Policy Loss: 0.0697, Avg VAE Loss: 0.1352, Avg Value Loss: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2106/9666 [00:55<03:17, 38.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2100: Avg Reward: 0.3200, Avg Policy Loss: 0.0650, Avg VAE Loss: 0.1706, Avg Value Loss: 0.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2206/9666 [00:58<03:18, 37.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2200: Avg Reward: 0.2300, Avg Policy Loss: 0.0981, Avg VAE Loss: 0.1662, Avg Value Loss: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2306/9666 [01:00<03:18, 37.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2300: Avg Reward: 0.2200, Avg Policy Loss: 0.0377, Avg VAE Loss: 0.1354, Avg Value Loss: 0.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2406/9666 [01:03<03:11, 37.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2400: Avg Reward: 0.2200, Avg Policy Loss: 0.0514, Avg VAE Loss: 0.1487, Avg Value Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2506/9666 [01:06<03:12, 37.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2500: Avg Reward: 0.2200, Avg Policy Loss: 0.0434, Avg VAE Loss: 0.1555, Avg Value Loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2607/9666 [01:08<03:06, 37.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2600: Avg Reward: 0.2300, Avg Policy Loss: 0.1280, Avg VAE Loss: 0.1429, Avg Value Loss: 0.0328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2707/9666 [01:11<03:13, 35.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2700: Avg Reward: 0.2200, Avg Policy Loss: 0.0697, Avg VAE Loss: 0.1348, Avg Value Loss: 0.0475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2807/9666 [01:14<03:08, 36.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2800: Avg Reward: 0.2500, Avg Policy Loss: 0.0522, Avg VAE Loss: 0.1418, Avg Value Loss: 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2907/9666 [01:17<03:08, 35.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2900: Avg Reward: 0.3000, Avg Policy Loss: 0.0396, Avg VAE Loss: 0.1423, Avg Value Loss: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3007/9666 [01:19<02:58, 37.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3000: Avg Reward: 0.2200, Avg Policy Loss: 0.0432, Avg VAE Loss: 0.1189, Avg Value Loss: 0.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3107/9666 [01:22<02:49, 38.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3100: Avg Reward: 0.2600, Avg Policy Loss: 0.0818, Avg VAE Loss: 0.1597, Avg Value Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3207/9666 [01:25<02:51, 37.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3200: Avg Reward: 0.2900, Avg Policy Loss: 0.0831, Avg VAE Loss: 0.1583, Avg Value Loss: 0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3307/9666 [01:27<02:43, 38.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3300: Avg Reward: 0.2200, Avg Policy Loss: 0.0563, Avg VAE Loss: 0.1337, Avg Value Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 3407/9666 [01:30<02:47, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3400: Avg Reward: 0.2300, Avg Policy Loss: 0.0692, Avg VAE Loss: 0.1240, Avg Value Loss: 0.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 3507/9666 [01:33<02:44, 37.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3500: Avg Reward: 0.3300, Avg Policy Loss: 0.0470, Avg VAE Loss: 0.1379, Avg Value Loss: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3607/9666 [01:35<02:42, 37.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3600: Avg Reward: 0.2200, Avg Policy Loss: 0.0540, Avg VAE Loss: 0.1654, Avg Value Loss: 0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3706/9666 [01:38<02:44, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3700: Avg Reward: 0.3300, Avg Policy Loss: 0.0322, Avg VAE Loss: 0.1323, Avg Value Loss: 0.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3806/9666 [01:41<02:38, 36.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3800: Avg Reward: 0.2200, Avg Policy Loss: 0.0611, Avg VAE Loss: 0.1221, Avg Value Loss: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 3906/9666 [01:44<02:37, 36.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3900: Avg Reward: 0.2200, Avg Policy Loss: 0.0874, Avg VAE Loss: 0.1391, Avg Value Loss: 0.0506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 4006/9666 [01:47<02:30, 37.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4000: Avg Reward: 0.2200, Avg Policy Loss: 0.0298, Avg VAE Loss: 0.1149, Avg Value Loss: 0.1046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4106/9666 [01:49<02:28, 37.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4100: Avg Reward: 0.3100, Avg Policy Loss: 0.0828, Avg VAE Loss: 0.1430, Avg Value Loss: 0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 4206/9666 [01:52<02:28, 36.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4200: Avg Reward: 0.2400, Avg Policy Loss: 0.0683, Avg VAE Loss: 0.1323, Avg Value Loss: 0.1258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4306/9666 [01:55<02:28, 36.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4300: Avg Reward: 0.3300, Avg Policy Loss: 0.0507, Avg VAE Loss: 0.1614, Avg Value Loss: 0.1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4406/9666 [01:58<02:25, 36.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4400: Avg Reward: 0.2100, Avg Policy Loss: 0.0505, Avg VAE Loss: 0.1213, Avg Value Loss: 0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4506/9666 [02:01<02:26, 35.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4500: Avg Reward: 0.2300, Avg Policy Loss: 0.1283, Avg VAE Loss: 0.1795, Avg Value Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4606/9666 [02:03<02:18, 36.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4600: Avg Reward: 0.1500, Avg Policy Loss: 0.0929, Avg VAE Loss: 0.1160, Avg Value Loss: 0.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 4706/9666 [02:06<02:14, 36.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4700: Avg Reward: 0.2900, Avg Policy Loss: 0.0592, Avg VAE Loss: 0.1390, Avg Value Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4806/9666 [02:09<02:12, 36.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4800: Avg Reward: 0.2200, Avg Policy Loss: 0.1084, Avg VAE Loss: 0.1468, Avg Value Loss: 0.1007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 4906/9666 [02:12<02:15, 35.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4900: Avg Reward: 0.2200, Avg Policy Loss: 0.1032, Avg VAE Loss: 0.1469, Avg Value Loss: 0.1247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5006/9666 [02:15<02:08, 36.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5000: Avg Reward: 0.2000, Avg Policy Loss: 0.0417, Avg VAE Loss: 0.1263, Avg Value Loss: 0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5106/9666 [02:17<02:05, 36.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5100: Avg Reward: 0.1300, Avg Policy Loss: 0.1388, Avg VAE Loss: 0.1558, Avg Value Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5206/9666 [02:20<01:58, 37.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5200: Avg Reward: 0.2700, Avg Policy Loss: 0.0742, Avg VAE Loss: 0.1463, Avg Value Loss: 0.1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 5306/9666 [02:23<01:59, 36.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5300: Avg Reward: 0.2800, Avg Policy Loss: 0.0653, Avg VAE Loss: 0.1416, Avg Value Loss: 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5406/9666 [02:26<01:58, 35.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5400: Avg Reward: 0.2200, Avg Policy Loss: 0.0533, Avg VAE Loss: 0.1080, Avg Value Loss: 0.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5506/9666 [02:28<01:53, 36.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5500: Avg Reward: 0.2200, Avg Policy Loss: 0.0581, Avg VAE Loss: 0.1228, Avg Value Loss: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5606/9666 [02:31<01:53, 35.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5600: Avg Reward: 0.2300, Avg Policy Loss: 0.1810, Avg VAE Loss: 0.1362, Avg Value Loss: 0.0558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5706/9666 [02:34<01:48, 36.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5700: Avg Reward: 0.2200, Avg Policy Loss: 0.0961, Avg VAE Loss: 0.1695, Avg Value Loss: 0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 5806/9666 [02:37<01:44, 36.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5800: Avg Reward: 0.1100, Avg Policy Loss: 0.0682, Avg VAE Loss: 0.1329, Avg Value Loss: 0.0566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 5906/9666 [02:39<01:43, 36.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5900: Avg Reward: 0.2200, Avg Policy Loss: 0.0401, Avg VAE Loss: 0.1122, Avg Value Loss: 0.0497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6006/9666 [02:42<01:52, 32.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6000: Avg Reward: 0.2200, Avg Policy Loss: 0.0517, Avg VAE Loss: 0.1296, Avg Value Loss: 0.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6106/9666 [02:45<01:39, 35.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6100: Avg Reward: 0.2200, Avg Policy Loss: 0.0354, Avg VAE Loss: 0.1284, Avg Value Loss: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6206/9666 [02:48<01:55, 30.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6200: Avg Reward: 0.2300, Avg Policy Loss: 0.1030, Avg VAE Loss: 0.1497, Avg Value Loss: 0.0643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 6306/9666 [02:51<01:34, 35.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6300: Avg Reward: 0.2200, Avg Policy Loss: 0.0936, Avg VAE Loss: 0.1472, Avg Value Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 6406/9666 [02:54<01:30, 35.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6400: Avg Reward: 0.2200, Avg Policy Loss: 0.0595, Avg VAE Loss: 0.1182, Avg Value Loss: 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6506/9666 [02:57<01:27, 36.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6500: Avg Reward: 0.2200, Avg Policy Loss: 0.0616, Avg VAE Loss: 0.1248, Avg Value Loss: 0.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6606/9666 [02:59<01:21, 37.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6600: Avg Reward: 0.2200, Avg Policy Loss: 0.0894, Avg VAE Loss: 0.1474, Avg Value Loss: 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6706/9666 [03:02<01:20, 36.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6700: Avg Reward: 0.2200, Avg Policy Loss: 0.0436, Avg VAE Loss: 0.1240, Avg Value Loss: 0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 6806/9666 [03:05<01:21, 35.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6800: Avg Reward: 0.2200, Avg Policy Loss: 0.0496, Avg VAE Loss: 0.1209, Avg Value Loss: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 6906/9666 [03:08<01:19, 34.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6900: Avg Reward: 0.2200, Avg Policy Loss: 0.0495, Avg VAE Loss: 0.1526, Avg Value Loss: 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7006/9666 [03:11<01:16, 34.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7000: Avg Reward: 0.2200, Avg Policy Loss: 0.0305, Avg VAE Loss: 0.1191, Avg Value Loss: 0.1232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 7106/9666 [03:14<01:13, 34.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7100: Avg Reward: 0.2200, Avg Policy Loss: 0.0866, Avg VAE Loss: 0.1302, Avg Value Loss: 0.1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 7206/9666 [03:16<01:06, 36.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7200: Avg Reward: 0.2200, Avg Policy Loss: 0.0453, Avg VAE Loss: 0.1281, Avg Value Loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7306/9666 [03:19<01:05, 35.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7300: Avg Reward: 0.2200, Avg Policy Loss: 0.0297, Avg VAE Loss: 0.1177, Avg Value Loss: 0.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7406/9666 [03:22<01:01, 36.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7400: Avg Reward: 0.2200, Avg Policy Loss: 0.1075, Avg VAE Loss: 0.1149, Avg Value Loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7506/9666 [03:25<00:57, 37.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7500: Avg Reward: 0.2200, Avg Policy Loss: 0.0418, Avg VAE Loss: 0.1282, Avg Value Loss: 0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 7606/9666 [03:27<00:56, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7600: Avg Reward: 0.2200, Avg Policy Loss: 0.0444, Avg VAE Loss: 0.1540, Avg Value Loss: 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 7706/9666 [03:30<00:52, 37.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7700: Avg Reward: 0.2200, Avg Policy Loss: 0.0696, Avg VAE Loss: 0.1579, Avg Value Loss: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 7806/9666 [03:33<00:53, 34.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7800: Avg Reward: 0.2600, Avg Policy Loss: 0.0697, Avg VAE Loss: 0.1285, Avg Value Loss: 0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 7906/9666 [03:36<00:49, 35.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7900: Avg Reward: 0.1800, Avg Policy Loss: 0.0609, Avg VAE Loss: 0.1334, Avg Value Loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8006/9666 [03:39<00:47, 35.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8000: Avg Reward: 0.3300, Avg Policy Loss: 0.0472, Avg VAE Loss: 0.1227, Avg Value Loss: 0.1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8106/9666 [03:42<00:44, 35.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8100: Avg Reward: 0.1100, Avg Policy Loss: 0.0282, Avg VAE Loss: 0.0913, Avg Value Loss: 0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 8206/9666 [03:44<00:39, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8200: Avg Reward: 0.2200, Avg Policy Loss: 0.0473, Avg VAE Loss: 0.1336, Avg Value Loss: 0.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8306/9666 [03:47<00:37, 36.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8300: Avg Reward: 0.2200, Avg Policy Loss: 0.0570, Avg VAE Loss: 0.1273, Avg Value Loss: 0.1214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8406/9666 [03:50<00:40, 31.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8400: Avg Reward: 0.2400, Avg Policy Loss: 0.0337, Avg VAE Loss: 0.1164, Avg Value Loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8504/9666 [03:53<00:32, 35.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8500: Avg Reward: 0.2300, Avg Policy Loss: 0.0292, Avg VAE Loss: 0.1253, Avg Value Loss: 0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8604/9666 [03:56<00:30, 35.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8600: Avg Reward: 0.1900, Avg Policy Loss: 0.1426, Avg VAE Loss: 0.1375, Avg Value Loss: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 8704/9666 [03:59<00:27, 34.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8700: Avg Reward: 0.2200, Avg Policy Loss: 0.1532, Avg VAE Loss: 0.1470, Avg Value Loss: 0.0976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 8804/9666 [04:02<00:24, 35.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8800: Avg Reward: 0.2200, Avg Policy Loss: 0.0940, Avg VAE Loss: 0.1037, Avg Value Loss: 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 8904/9666 [04:05<00:22, 33.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8900: Avg Reward: 0.2200, Avg Policy Loss: 0.0986, Avg VAE Loss: 0.1310, Avg Value Loss: 0.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9004/9666 [04:07<00:19, 34.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9000: Avg Reward: 0.2200, Avg Policy Loss: 0.0439, Avg VAE Loss: 0.1232, Avg Value Loss: 0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9104/9666 [04:10<00:15, 37.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9100: Avg Reward: 0.2200, Avg Policy Loss: 0.1086, Avg VAE Loss: 0.1337, Avg Value Loss: 0.1893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 9204/9666 [04:13<00:12, 36.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9200: Avg Reward: 0.2200, Avg Policy Loss: 0.0593, Avg VAE Loss: 0.1216, Avg Value Loss: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 9304/9666 [04:16<00:10, 35.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9300: Avg Reward: 0.2400, Avg Policy Loss: 0.0615, Avg VAE Loss: 0.1486, Avg Value Loss: 0.2497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9404/9666 [04:18<00:07, 36.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9400: Avg Reward: 0.3100, Avg Policy Loss: 0.1524, Avg VAE Loss: 0.1244, Avg Value Loss: 0.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9504/9666 [04:21<00:04, 35.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9500: Avg Reward: 0.2200, Avg Policy Loss: 0.0479, Avg VAE Loss: 0.1249, Avg Value Loss: 0.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9604/9666 [04:24<00:01, 33.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9600: Avg Reward: 0.2200, Avg Policy Loss: 0.0518, Avg VAE Loss: 0.1413, Avg Value Loss: 0.1955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9666/9666 [04:26<00:00, 36.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, data_loader, M=8, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train the IRIS algorithm over full trajectories using a DataLoader that provides batches of trajectories.\n",
    "\n",
    "    Args:\n",
    "        low_level_policy (nn.Module): Low-level policy network.\n",
    "        goal_proposal_vae (nn.Module): Goal proposal VAE network.\n",
    "        action_vae (nn.Module): Action proposal VAE network.\n",
    "        value_network (nn.Module): Value network.\n",
    "        data_loader (DataLoader): DataLoader that provides batches of trajectories.\n",
    "        M (int): Number of samples for action sampling.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move models to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=1e-4)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=1e-4)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=1e-4)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=1e-4)\n",
    "\n",
    "    # Statistics for monitoring\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "\n",
    "    # Iterate through batches of trajectories\n",
    "    for epoch in range(1):\n",
    "        for batch_idx, trajectory_batch in enumerate(tqdm(data_loader)):\n",
    "            # Each element in trajectory_batch is a batch of sequences\n",
    "            batch_size = trajectory_batch['obs']['robot0_eef_pos'].shape[0]\n",
    "            for i in range(batch_size):\n",
    "                # Extract the i-th trajectory from the batch\n",
    "                states = torch.cat([\n",
    "                    # End-effector position (3,)\n",
    "                    trajectory_batch['obs']['robot0_eef_pos'][i],\n",
    "                    # Object-related observation (10,)\n",
    "                    trajectory_batch['obs']['object'][i],\n",
    "                    # End-effector orientation (4,)\n",
    "                    trajectory_batch['obs']['robot0_eef_quat'][i],\n",
    "                    # Gripper position (2,)\n",
    "                    # trajectory_batch['obs']['robot0_gripper_qpos'][i],\n",
    "                    # Gripper velocity (2,)\n",
    "                    # trajectory_batch['obs']['robot0_gripper_qvel'][i],\n",
    "                    # 7DOF joint positions (7,)\n",
    "                    # trajectory_batch['obs']['robot0_joint_pos_cos'][i],\n",
    "                    # trajectory_batch['obs']['robot0_joint_pos_sin'][i],\n",
    "                    # 7DOF joint velocities (7,)\n",
    "                    # trajectory_batch['obs']['robot0_joint_vel'][i]\n",
    "                ], axis=1).to(device)  # Concatenate along the feature axis\n",
    "                # print(states.shape)\n",
    "                # states = torch.cat([\n",
    "                #     trajectory_batch['obs']['object'][i],\n",
    "                # ], axis=1).to(device)  # Concatenate along the feature axis\n",
    "                states = torch.squeeze(states.type(torch.float32))\n",
    "\n",
    "                actions = trajectory_batch['actions'][i].to(device)\n",
    "                actions = torch.squeeze(actions.type(torch.float32))\n",
    "\n",
    "                rewards = trajectory_batch['rewards'][i].to(device)\n",
    "                rewards = torch.squeeze(rewards.type(torch.float32))\n",
    "\n",
    "                # Remove last timestep for actions (to match states length)\n",
    "                actions = actions[:-1]\n",
    "\n",
    "                sg = states[-1]\n",
    "                s_start = states[0]\n",
    "                reward_sg = rewards[-2]\n",
    "                action_last = actions[-2]\n",
    "                state_second_last = states[-2]\n",
    "\n",
    "                # Train Low-Level Policy\n",
    "                policy_actions = [torch.squeeze(low_level_policy(\n",
    "                    state, sg)) for state in states[:-1]]\n",
    "                policy_actions = torch.squeeze(torch.stack(policy_actions))\n",
    "                policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "\n",
    "                policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                policy_optimizer.step()\n",
    "\n",
    "                # VAE updates (Goal and Action VAEs)\n",
    "                mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "                z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "                vae_loss = nn.MSELoss()(sg, goal_proposal_vae.decode(z, s_start)) + \\\n",
    "                    0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1)\n",
    "\n",
    "                mu_a, logvar_a = action_vae.encode(\n",
    "                    action_last, state_second_last)\n",
    "                z_a = action_vae.reparameterize(mu_a, logvar_a)\n",
    "                action_vae_loss = nn.MSELoss()(action_last, action_vae.decode(z_a, state_second_last)) + \\\n",
    "                    0.5 * torch.sum(mu_a.pow(2) +\n",
    "                                    logvar_a.exp() - logvar_a - 1)\n",
    "\n",
    "                vae_optimizer.zero_grad()\n",
    "                action_optimizer.zero_grad()\n",
    "                (vae_loss + action_vae_loss).backward()\n",
    "                vae_optimizer.step()\n",
    "                action_optimizer.step()\n",
    "\n",
    "                # Sample actions and update value network\n",
    "                sampled_actions = [action_vae.decode(\n",
    "                    z_a, sg) for _ in range(M)]\n",
    "                sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "                values = torch.stack([value_network(sg, a)\n",
    "                                      for a in sampled_actions])\n",
    "                max_value = torch.max(values)\n",
    "\n",
    "                Vbar = torch.squeeze(reward_sg + gamma * max_value.detach())\n",
    "                value_loss = nn.MSELoss()(Vbar, torch.squeeze(\n",
    "                    value_network(state_second_last, action_last)))\n",
    "\n",
    "                value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                value_optimizer.step()\n",
    "\n",
    "                # Store losses and rewards for monitoring\n",
    "                cumulative_rewards.append(reward_sg.item())\n",
    "                cumulative_policy_loss.append(policy_loss.item())\n",
    "                cumulative_vae_loss.append(\n",
    "                    vae_loss.item() + action_vae_loss.item())\n",
    "                cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "            # Print statistics every N batches (you can adjust this interval)\n",
    "            if (batch_idx+1) % 100 == 0:\n",
    "                avg_reward = np.mean(cumulative_rewards)\n",
    "                avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "                avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "                avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "                print(f\"Batch {batch_idx + 1}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                      f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                      f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                      f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "                # Reset cumulative stats after printing\n",
    "                cumulative_rewards = []\n",
    "                cumulative_policy_loss = []\n",
    "                cumulative_vae_loss = []\n",
    "                cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully as .pth files.\n"
     ]
    }
   ],
   "source": [
    "# Save each model\n",
    "torch.save(low_level_policy.state_dict(), 'low_level_policy.pth')\n",
    "torch.save(goal_proposal_vae.state_dict(), 'goal_proposal_vae.pth')\n",
    "torch.save(action_vae.state_dict(), 'action_vae.pth')\n",
    "torch.save(value_network.state_dict(), 'value_network.pth')\n",
    "\n",
    "print(\"Models saved successfully as .pth files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37898/953007346.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  low_level_policy.load_state_dict(torch.load('low_level_policy.pth'))\n",
      "/tmp/ipykernel_37898/953007346.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  goal_proposal_vae.load_state_dict(torch.load('goal_proposal_vae.pth'))\n",
      "/tmp/ipykernel_37898/953007346.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  action_vae.load_state_dict(torch.load('action_vae.pth'))\n",
      "/tmp/ipykernel_37898/953007346.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  value_network.load_state_dict(torch.load('value_network.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_level_policy.load_state_dict(torch.load('low_level_policy.pth'))\n",
    "goal_proposal_vae.load_state_dict(torch.load('goal_proposal_vae.pth'))\n",
    "action_vae.load_state_dict(torch.load('action_vae.pth'))\n",
    "value_network.load_state_dict(torch.load('value_network.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/keerthi/.local/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n"
     ]
    }
   ],
   "source": [
    "import robosuite\n",
    "from robosuite.controllers import load_controller_config\n",
    "\n",
    "# load default controller parameters for Operational Space Control (OSC)\n",
    "controller_config = load_controller_config(default_controller=\"OSC_POSE\")\n",
    "# controller_config['uncouple_pos_ori'] = False\n",
    "# print(controller_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(state):\n",
    "    # Extract and concatenate the desired features from the state\n",
    "    # features = torch.cat([\n",
    "    #     # End-effector position (3,)\n",
    "    #     torch.tensor(state['robot0_eef_pos'], dtype=torch.float32),\n",
    "    #     # Object-related observation (10,)\n",
    "    #     torch.tensor(state['object-state'], dtype=torch.float32),\n",
    "    #     # End-effector orientation (4,)\n",
    "    #     torch.tensor(state['robot0_eef_quat'], dtype=torch.float32),\n",
    "    #     # Gripper position (2,)\n",
    "    #     torch.tensor(state['robot0_gripper_qpos'], dtype=torch.float32),\n",
    "    #     # Gripper velocity (2,)\n",
    "    #     torch.tensor(state['robot0_gripper_qvel'], dtype=torch.float32),\n",
    "    #     # 7DOF joint positions (7,)\n",
    "    #     torch.tensor(\n",
    "    #         state['robot0_joint_pos_cos'], dtype=torch.float32),\n",
    "    #     # 7DOF joint velocities (7,)\n",
    "    #     torch.tensor(state['robot0_joint_vel'], dtype=torch.float32)\n",
    "    # ], dim=0)  # Concatenate into a single 1D tensor\n",
    "    features = torch.cat([\n",
    "        # End-effector position (3,)\n",
    "        torch.tensor(state['robot0_eef_pos'], dtype=torch.float32),\n",
    "        # Object-related observation (10,)\n",
    "        torch.tensor(state['object-state'], dtype=torch.float32),\n",
    "        # End-effector orientation (4,)\n",
    "        torch.tensor(state['robot0_eef_quat'], dtype=torch.float32),\n",
    "        # Gripper position (2,)\n",
    "        # torch.tensor(state['robot0_gripper_qpos'], dtype=torch.float32),\n",
    "        # Gripper velocity (2,)\n",
    "        # torch.tensor(state['robot0_gripper_qvel'], dtype=torch.float32),\n",
    "        # 7DOF joint positions (7,)\n",
    "        # torch.tensor(\n",
    "        # state['robot0_joint_pos_cos'], dtype=torch.float32),\n",
    "        # torch.tensor(state['robot0_joint_pos_sin'], dtype=torch.float32),\n",
    "        # 7DOF joint velocities (7,)\n",
    "        # torch.tensor(state['robot0_joint_vel'], dtype=torch.float32)\n",
    "    ], dim=0)  # Concatenate into a single 1D tensor\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import robosuite as suite\n",
    "import cv2\n",
    "# create environment instance\n",
    "env = suite.make(\n",
    "    env_name=\"Lift\",  # try with other tasks like \"Stack\" and \"Door\"\n",
    "    robots=\"Sawyer\",  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    ")\n",
    "env.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example cosine and sine values for each joint\n",
    "\n",
    "# joint_pos_cos = goal_state_config[20:27].detach().cpu().numpy()\n",
    "# joint_pos_sin = goal_state_config[27:34].detach().cpu().numpy()\n",
    "# custom_cube_position = np.array([0.5, 0.0, 0.02])  # Position in (x, y, z)\n",
    "# custom_cube_orientation = np.array([1, 0, 0, 0])   # Quaternion (w, x, y, z)\n",
    "# reset_with_cos_sin(env, joint_pos_cos, joint_pos_sin,\n",
    "#                    custom_cube_position, custom_cube_orientation)\n",
    "\n",
    "# # Access observables after custom reset\n",
    "# observations = env._get_observations()\n",
    "# # print(observations)\n",
    "# # for obs_name in ['cube_pos', 'cube_quat', 'robot0_joint_pos']:\n",
    "# #     print(f\"{obs_name}: {observations[obs_name]}\")\n",
    "\n",
    "# # Render to visualize the environment\n",
    "# while True:\n",
    "#     # _, _, _, _ = env.step([0, 0, 0, 0, 0, 0, 0, 0.5])\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0298,  0.0221,  0.8355,  0.0237,  0.0270,  0.8314, -0.0080, -0.0045,\n",
       "         0.9694,  0.2452,  0.0061, -0.0049,  0.0041,  0.9862, -0.1468,  0.0732,\n",
       "         0.0202], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "goal_state_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.0452e-01,  4.3512e-01,  8.1461e-01,  4.2443e-03,  3.7202e-03,\n",
       "         8.2142e-01,  3.7909e-17,  5.9400e-18,  1.3081e-01,  9.9141e-01,\n",
       "        -5.0877e-01,  4.3140e-01, -6.8032e-03,  7.0655e-01, -6.4116e-01,\n",
       "        -2.9877e-01, -2.0888e-02])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor([-0.0984,  0.1381,  1.0242,  0.0256, -0.0211,  0.8311,  0.0000,  0.0000,\n",
    "#          0.9863, -0.1648, -0.1240,  0.1592,  0.1931,  0.9967,  0.0800,  0.0082,\n",
    "#          0.0115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37898/1297916813.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
      "/tmp/ipykernel_37898/1297916813.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "/tmp/ipykernel_37898/1297916813.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[171], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     62\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mlow_level_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# height, width, _ = env.render().shape\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "# video_writer = cv2.VideoWriter(\n",
    "# \"env_policy_video_lift.mp4\", fourcc, 30, (width, height))\n",
    "goal = goal_state_config.to(\"cpu\")\n",
    "# goal[3:6]=torch.zeros(3)\n",
    "# goal = torch.zeros_like(goal_state_config)\n",
    "# goal[10:13] = torch.zeros(3)\n",
    "# # goal[10]=4.1\n",
    "# # goal[11]=4\n",
    "# goal[12]=4\n",
    "# goal[7:] = goal_req\n",
    "\n",
    "state = extract_features(env.reset())\n",
    "desired_position = np.array([-0.02, 0.222, 2])\n",
    "\n",
    "# Find the cube's joint name in the environment\n",
    "cube_name = \"cube_main\"\n",
    "cube_joint = env.sim.model.body_name2id(cube_name)\n",
    "\n",
    "# Set the position of the cube\n",
    "env.sim.data.set_joint_qpos(\n",
    "    f\"cube_joint0\", np.concatenate([desired_position, np.zeros(4)]))\n",
    "\n",
    "# Ensure the simulation state is updated\n",
    "env.sim.forward()\n",
    "# goal = state\n",
    "# print(state)\n",
    "value_function = value_network.to(\"cpu\")\n",
    "goal_proposal_vae.to(\"cpu\")\n",
    "action_vae.to(\"cpu\")\n",
    "low_level_policy.to(\"cpu\")\n",
    "\n",
    "for step in range(1000):\n",
    "    # Render the environment and capture the frame\n",
    "    frame = state\n",
    "    # Write frame to video\n",
    "    # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "    # VAE update\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "    z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "    goal_final = goal\n",
    "    # value = value_function(goal, torch.squeeze(\n",
    "    #     low_level_policy(state_tensor, goal)))\n",
    "    # for k in range(10):\n",
    "    #     z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    #     goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "    #     if value_function(goal, torch.squeeze(low_level_policy(state_tensor, goal))) > value:\n",
    "    #         goal_final = goal\n",
    "    #         value = value_function(\n",
    "    #             goal, torch.squeeze(low_level_policy(state_tensor, goal)))\n",
    "    goal = goal_final\n",
    "    next_state = None\n",
    "    # Get the action from the low-level policy\n",
    "    reward = 0\n",
    "    for _ in range(1):\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = low_level_policy(\n",
    "            state_tensor, goal).detach().numpy()\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        action = np.squeeze(action)\n",
    "        # First joint is not activated\n",
    "        next_state, reward, done, _ = env.step(\n",
    "            np.concatenate((np.array([0]), action)))\n",
    "        # frame = env.render(mode=\"rgb_array\")\n",
    "        env.render()\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        state = extract_features(next_state)\n",
    "    #     if reward > 0:\n",
    "    #         print(\n",
    "    #             f\"Episode {step+1}, Step {step+1}: Goal reached!\")\n",
    "    #         break\n",
    "    #     if done:\n",
    "    #         print(state)\n",
    "    #         break  # Terminate the episode if done is True\n",
    "    # if done:\n",
    "    #     break\n",
    "    # if reward > 0:\n",
    "    #     break\n",
    "# for i in range(10):     frame = env.render()\n",
    "#     state = env.reset()\n",
    "#     goal = state\n",
    "#     state=extract_features(state)\n",
    "#     action = np.random.randn(env.robots[0].dof)  # sample random action\n",
    "#     # take action in the environment\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     env.render()  # render on display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37898/1355582020.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
      "/tmp/ipykernel_37898/1355582020.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "/tmp/ipykernel_37898/1355582020.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as env_policy_video_lift.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define video dimensions and initialize video writer\n",
    "width, height = 640, 480\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"env_policy_video_lift_work3.mp4\", fourcc, 30, (width, height))\n",
    "\n",
    "goal = goal_state_config.to(\"cpu\")\n",
    "# goal[12] = 4\n",
    "# goal[7:] = goal_req\n",
    "goal = goal_state_config.to(\"cpu\")\n",
    "# goal[3:6]=torch.zeros(3)\n",
    "# goal = torch.zeros_like(goal_state_config)\n",
    "# goal[10:13] = torch.zeros(3)\n",
    "# # goal[10]=4.1\n",
    "# # goal[11]=4\n",
    "# goal[12]=4\n",
    "# goal[7:] = goal_req\n",
    "\n",
    "state = extract_features(env.reset())\n",
    "# desired_position = np.array([-0.120, 0.228, 0.8])\n",
    "desired_position = np.array([-0.04, 0.24, 0.8])\n",
    "\n",
    "# Find the cube's joint name in the environment\n",
    "cube_name = \"cube_main\"\n",
    "cube_joint = env.sim.model.body_name2id(cube_name)\n",
    "\n",
    "# Set the position of the cube\n",
    "env.sim.data.set_joint_qpos(\n",
    "    f\"cube_joint0\", np.concatenate([desired_position, np.zeros(4)]))\n",
    "\n",
    "# Ensure the simulation state is updated\n",
    "env.sim.forward()\n",
    "\n",
    "# Move models to CPU\n",
    "value_function = value_network.to(\"cpu\")\n",
    "goal_proposal_vae.to(\"cpu\")\n",
    "action_vae.to(\"cpu\")\n",
    "low_level_policy.to(\"cpu\")\n",
    "\n",
    "# state = extract_features(env.reset())  # Initial state\n",
    "\n",
    "for step in range(1000):\n",
    "    # Render from camera directly in robosuite\n",
    "    frame = env.sim.render(camera_name=\"frontview\", width=width, height=height)\n",
    "    frame = np.flipud(frame)  # Flip frame if needed for orientation\n",
    "    if frame is not None:\n",
    "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    env.render()\n",
    "    # VAE operations\n",
    "    mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "    z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "    goal_final = goal\n",
    "    value = value_function(goal, torch.squeeze(\n",
    "        low_level_policy(state_tensor, goal)))\n",
    "\n",
    "    # for k in range(10):\n",
    "    #     z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    #     goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "    #     current_value = value_function(goal, torch.squeeze(\n",
    "    #         low_level_policy(state_tensor, goal)))\n",
    "    #     if current_value > value:\n",
    "    #         goal_final = goal\n",
    "    #         value = current_value\n",
    "\n",
    "    goal = goal_final  # Set optimized goal\n",
    "    next_state = None\n",
    "\n",
    "    # Execute actions\n",
    "    for _ in range(3):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = low_level_policy(state_tensor, goal).detach().numpy()\n",
    "        action = np.squeeze(action)\n",
    "\n",
    "        # Apply action and capture new frame\n",
    "        next_state, reward, done, _ = env.step(\n",
    "            np.concatenate((np.array([0]), action)))\n",
    "        frame = env.sim.render(camera_name=\"frontview\",\n",
    "                               width=width, height=height)\n",
    "        frame = np.flipud(frame)\n",
    "        if frame is not None:\n",
    "            video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        state = extract_features(next_state)\n",
    "\n",
    "    #     if reward > 0:\n",
    "    #         print(f\"Episode {step + 1}, Step {step + 1}: Goal reached!\")\n",
    "    #         break\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "# Release video writer\n",
    "video_writer.release()\n",
    "print(\"Video saved as env_policy_video_lift.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
