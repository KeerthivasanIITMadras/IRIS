{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import robomimic\n",
    "import robomimic.utils.obs_utils as ObsUtils\n",
    "import robomimic.utils.torch_utils as TorchUtils\n",
    "import robomimic.utils.test_utils as TestUtils\n",
    "import robomimic.utils.file_utils as FileUtils\n",
    "import robomimic.utils.train_utils as TrainUtils\n",
    "from robomimic.utils.dataset import SequenceDataset\n",
    "\n",
    "\n",
    "def get_data_loader(dataset_path, trajectory_length=50, batch_size=64):\n",
    "    \"\"\"\n",
    "    Get a data loader to sample batches of data.\n",
    "    Args:\n",
    "        dataset_path (str): path to the dataset hdf5\n",
    "        trajectory_length (int): number of timesteps in a trajectory\n",
    "        batch_size (int): batch size for the DataLoader\n",
    "    \"\"\"\n",
    "    dataset = SequenceDataset(\n",
    "        hdf5_path=dataset_path,\n",
    "        obs_keys=(                      # observations we want to appear in batches\n",
    "            \"robot0_eef_pos\",\n",
    "            \"robot0_eef_quat\",\n",
    "            \"robot0_gripper_qpos\",\n",
    "            \"object\",\n",
    "            \"robot0_gripper_qvel\",       # Additional gripper velocity\n",
    "            \"robot0_joint_pos\",          # 7DOF joint positions\n",
    "            \"robot0_joint_vel\",          # 7DOF joint velocities\n",
    "        ),\n",
    "        dataset_keys=(                  # can optionally specify more keys here if they should appear in batches\n",
    "            \"actions\",\n",
    "            \"rewards\",\n",
    "            \"dones\",\n",
    "            \"horizon\",                  # Additional metadata\n",
    "            \"episode_id\",               # Episode information\n",
    "        ),\n",
    "        load_next_obs=True,             # load the next observation in each sequence\n",
    "        frame_stack=1,\n",
    "        seq_length=trajectory_length,   # length-10 temporal sequences\n",
    "        # pad last obs per trajectory to ensure all sequences are sampled\n",
    "        pad_frame_stack=True,\n",
    "        pad_seq_length=True,            # pad last observation sequence\n",
    "        get_pad_mask=False,             # do not return padding masks\n",
    "        goal_mode=None,\n",
    "        hdf5_cache_mode=\"all\",          # cache dataset in memory to avoid repeated file i/o\n",
    "        hdf5_use_swmr=True,\n",
    "        hdf5_normalize_obs=False,       # normalize observations\n",
    "        filter_by_attribute=None,       # can optionally provide a filter key here\n",
    "    )\n",
    "\n",
    "    print(\"\\n============= Created Dataset =============\")\n",
    "    print(dataset)\n",
    "    # Print additional dataset information\n",
    "    print(f\"Dataset length: {len(dataset)}\")\n",
    "    print(f\"Observation keys: {dataset.obs_keys}\")\n",
    "    print(f\"Dataset keys: {dataset.dataset_keys}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Set batch size to a manageable number\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        # no custom sampling logic (uniform sampling)\n",
    "        sampler=None,\n",
    "        batch_size=batch_size,          # batches of size 32\n",
    "        shuffle=True,                   # shuffle the dataset\n",
    "        # use 0 workers (could be set to more if using multiprocessing)\n",
    "        num_workers=0,\n",
    "        drop_last=True                  # drop the last incomplete batch\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceDataset: loading dataset into memory...\n",
      "100%|██████████| 200/200 [00:00<00:00, 451.60it/s]\n",
      "SequenceDataset: caching get_item calls...\n",
      "100%|██████████| 9666/9666 [00:02<00:00, 4683.54it/s]\n",
      "\n",
      "============= Created Dataset =============\n",
      "SequenceDataset (\n",
      "\tpath=/home/keerthi/IRIS/lift/ph/low_dim_v141.hdf5\n",
      "\tobs_keys=('robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'object', 'robot0_gripper_qvel', 'robot0_joint_pos', 'robot0_joint_vel')\n",
      "\tseq_length=50\n",
      "\tfilter_key=none\n",
      "\tframe_stack=1\n",
      "\tpad_seq_length=True\n",
      "\tpad_frame_stack=True\n",
      "\tgoal_mode=none\n",
      "\tcache_mode=all\n",
      "\tnum_demos=200\n",
      "\tnum_sequences=9666\n",
      ")\n",
      "Dataset length: 9666\n",
      "Observation keys: ('robot0_eef_pos', 'robot0_eef_quat', 'robot0_gripper_qpos', 'object', 'robot0_gripper_qvel', 'robot0_joint_pos', 'robot0_joint_vel')\n",
      "Dataset keys: ('actions', 'rewards', 'dones', 'horizon', 'episode_id')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_loader = get_data_loader(\n",
    "    dataset_path=\"/home/keerthi/IRIS/lift/ph/low_dim_v141.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 16/151 [00:49<06:55,  3.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m value_network \u001b[38;5;241m=\u001b[39m ValueNetwork(state_dim, action_dim)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Train the IRIS algorithm using the D4RL dataset\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m \u001b[43mtrain_IRIS_full_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlow_level_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_proposal_vae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43maction_vae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 123\u001b[0m, in \u001b[0;36mtrain_IRIS_full_trajectory\u001b[0;34m(low_level_policy, goal_proposal_vae, action_vae, value_network, data_loader, M, gamma)\u001b[0m\n\u001b[1;32m    121\u001b[0m value_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    122\u001b[0m value_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 123\u001b[0m \u001b[43mvalue_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Store losses and rewards for monitoring\u001b[39;00m\n\u001b[1;32m    126\u001b[0m cumulative_rewards\u001b[38;5;241m.\u001b[39mappend(reward_sg\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:518\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[0;32m--> 518\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, data_loader, M=15, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train the IRIS algorithm over full trajectories using a DataLoader that provides batches of trajectories.\n",
    "\n",
    "    Args:\n",
    "        low_level_policy (nn.Module): Low-level policy network.\n",
    "        goal_proposal_vae (nn.Module): Goal proposal VAE network.\n",
    "        action_vae (nn.Module): Action proposal VAE network.\n",
    "        value_network (nn.Module): Value network.\n",
    "        data_loader (DataLoader): DataLoader that provides batches of trajectories.\n",
    "        M (int): Number of samples for action sampling.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move models to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.0001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.0001)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.0001)\n",
    "\n",
    "    # Statistics for monitoring\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "\n",
    "    # Iterate through batches of trajectories\n",
    "    for epoch in range(1):\n",
    "        for batch_idx, trajectory_batch in enumerate(tqdm(data_loader)):\n",
    "            # Each element in trajectory_batch is a batch of sequences\n",
    "            batch_size = trajectory_batch['obs']['robot0_eef_pos'].shape[0]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Extract the i-th trajectory from the batch\n",
    "                states = torch.cat([\n",
    "                    # End-effector position (3,)\n",
    "                    trajectory_batch['obs']['robot0_eef_pos'][i],\n",
    "                    # Object-related observation (10,)\n",
    "                    trajectory_batch['obs']['object'][i],\n",
    "                    # End-effector orientation (4,)\n",
    "                    trajectory_batch['obs']['robot0_eef_quat'][i],\n",
    "                    # Gripper position (2,)\n",
    "                    trajectory_batch['obs']['robot0_gripper_qpos'][i],\n",
    "                    # Gripper velocity (2,)\n",
    "                    trajectory_batch['obs']['robot0_gripper_qvel'][i],\n",
    "                    # 7DOF joint positions (7,)\n",
    "                    trajectory_batch['obs']['robot0_joint_pos'][i],\n",
    "                    # 7DOF joint velocities (7,)\n",
    "                    trajectory_batch['obs']['robot0_joint_vel'][i]\n",
    "                ], axis=1).to(device)  # Concatenate along the feature axis\n",
    "\n",
    "                states = torch.squeeze(states.type(torch.float32))\n",
    "\n",
    "                actions = trajectory_batch['actions'][i].to(device)\n",
    "                actions = torch.squeeze(actions.type(torch.float32))\n",
    "\n",
    "                rewards = trajectory_batch['rewards'][i].to(device)\n",
    "                rewards = torch.squeeze(rewards.type(torch.float32))\n",
    "\n",
    "                # Remove last timestep for actions (to match states length)\n",
    "                actions = actions[:-1]\n",
    "\n",
    "                sg = states[-1]\n",
    "                s_start = states[0]\n",
    "                reward_sg = rewards[-2]\n",
    "                action_last = actions[-2]\n",
    "                state_second_last = states[-2]\n",
    "\n",
    "                # Train Low-Level Policy\n",
    "                policy_actions = [torch.squeeze(low_level_policy(\n",
    "                    state, sg)) for state in states[:-1]]\n",
    "                policy_actions = torch.squeeze(torch.stack(policy_actions))\n",
    "                policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "\n",
    "                policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                policy_optimizer.step()\n",
    "\n",
    "                # VAE updates (Goal and Action VAEs)\n",
    "                mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "                z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "                vae_loss = nn.MSELoss()(sg, goal_proposal_vae.decode(z, s_start)) + \\\n",
    "                    0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1)\n",
    "\n",
    "                mu_a, logvar_a = action_vae.encode(\n",
    "                    action_last, state_second_last)\n",
    "                z_a = action_vae.reparameterize(mu_a, logvar_a)\n",
    "                action_vae_loss = nn.MSELoss()(action_last, action_vae.decode(z_a, state_second_last)) + \\\n",
    "                    0.5 * torch.sum(mu_a.pow(2) +\n",
    "                                    logvar_a.exp() - logvar_a - 1)\n",
    "\n",
    "                vae_optimizer.zero_grad()\n",
    "                action_optimizer.zero_grad()\n",
    "                (vae_loss + action_vae_loss).backward()\n",
    "                vae_optimizer.step()\n",
    "                action_optimizer.step()\n",
    "\n",
    "                # Sample actions and update value network\n",
    "                sampled_actions = [action_vae.decode(\n",
    "                    z_a, sg) for _ in range(M)]\n",
    "                sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "                values = torch.stack([value_network(sg, a)\n",
    "                                      for a in sampled_actions])\n",
    "                max_value = torch.max(values)\n",
    "\n",
    "                Vbar = torch.squeeze(reward_sg + gamma * max_value.detach())\n",
    "                value_loss = nn.MSELoss()(Vbar, torch.squeeze(\n",
    "                    value_network(state_second_last, action_last)))\n",
    "\n",
    "                value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                value_optimizer.step()\n",
    "\n",
    "                # Store losses and rewards for monitoring\n",
    "                cumulative_rewards.append(reward_sg.item())\n",
    "                cumulative_policy_loss.append(policy_loss.item())\n",
    "                cumulative_vae_loss.append(\n",
    "                    vae_loss.item() + action_vae_loss.item())\n",
    "                cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "            # Print statistics every N batches (you can adjust this interval)\n",
    "            if (batch_idx+1) % 100 == 0:\n",
    "                avg_reward = np.mean(cumulative_rewards)\n",
    "                avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "                avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "                avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "                print(f\"Batch {batch_idx + 1}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                      f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                      f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                      f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "                # Reset cumulative stats after printing\n",
    "                cumulative_rewards = []\n",
    "                cumulative_policy_loss = []\n",
    "                cumulative_vae_loss = []\n",
    "                cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = 35\n",
    "state_goal_dim = 35\n",
    "action_dim = 7\n",
    "latent_dim = 20\n",
    "\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mNo private macro file found! (macros.py:53)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mIt is recommended to use a private macro file (macros.py:54)\n",
      "\u001b[1m\u001b[33m[robosuite WARNING] \u001b[0mTo setup, run: python /home/keerthi/.local/lib/python3.10/site-packages/robosuite/scripts/setup_macros.py (macros.py:55)\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import robosuite\n",
    "from robosuite.controllers import load_controller_config\n",
    "\n",
    "# load default controller parameters for Operational Space Control (OSC)\n",
    "controller_config = load_controller_config(default_controller=\"OSC_POSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(state):\n",
    "    # Extract and concatenate the desired features from the state\n",
    "    features = torch.cat([\n",
    "        # End-effector position (3,)\n",
    "        torch.tensor(state['robot0_eef_pos'], dtype=torch.float32),\n",
    "        # Object-related observation (10,)\n",
    "        torch.tensor(state['object-state'], dtype=torch.float32),\n",
    "        # End-effector orientation (4,)\n",
    "        torch.tensor(state['robot0_eef_quat'], dtype=torch.float32),\n",
    "        # Gripper position (2,)\n",
    "        torch.tensor(state['robot0_gripper_qpos'], dtype=torch.float32),\n",
    "        # Gripper velocity (2,)\n",
    "        torch.tensor(state['robot0_gripper_qvel'], dtype=torch.float32),\n",
    "        # 7DOF joint positions (7,)\n",
    "        torch.tensor(\n",
    "            np.arccos(state['robot0_joint_pos_cos']), dtype=torch.float32),\n",
    "        # 7DOF joint velocities (7,)\n",
    "        torch.tensor(state['robot0_joint_vel'], dtype=torch.float32)\n",
    "    ], dim=0)  # Concatenate into a single 1D tensor\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5575/605081698.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
      "/tmp/ipykernel_5575/605081698.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n",
      "/tmp/ipykernel_5575/605081698.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import robosuite as suite\n",
    "import cv2\n",
    "# create environment instance\n",
    "env = suite.make(\n",
    "    env_name=\"Lift\",  # try with other tasks like \"Stack\" and \"Door\"\n",
    "    robots=\"Sawyer\",  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    ")\n",
    "\n",
    "# height, width, _ = env.render().shape\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 video\n",
    "# video_writer = cv2.VideoWriter(\n",
    "# \"env_policy_video_lift.mp4\", fourcc, 30, (width, height))\n",
    "goal = extract_features(env.reset())\n",
    "state = extract_features(env.reset())\n",
    "value_function = value_network.to(\"cpu\")\n",
    "goal_proposal_vae.to(\"cpu\")\n",
    "action_vae.to(\"cpu\")\n",
    "low_level_policy.to(\"cpu\")\n",
    "\n",
    "for step in range(1000):\n",
    "    # Render the environment and capture the frame\n",
    "    frame = state\n",
    "    # Write frame to video\n",
    "    # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "    # VAE update\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "    z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "    goal_final = goal\n",
    "    value = value_function(goal, torch.squeeze(\n",
    "        low_level_policy(state_tensor, goal)))\n",
    "    for k in range(5):\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "        if value_function(goal, torch.squeeze(low_level_policy(state_tensor, goal))) > value:\n",
    "            goal_final = goal\n",
    "            value = value_function(\n",
    "                goal, torch.squeeze(low_level_policy(state_tensor, goal)))\n",
    "    goal = goal_final\n",
    "    next_state = None\n",
    "    # Get the action from the low-level policy\n",
    "    reward = 0\n",
    "    for _ in range(2):\n",
    "\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = low_level_policy(\n",
    "            state_tensor, goal).detach().numpy()\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        action = np.squeeze(action)\n",
    "        next_state, reward, done, _ = env.step(\n",
    "            np.concatenate((action, np.array([action[6]]))))\n",
    "        # frame = env.render(mode=\"rgb_array\")\n",
    "        env.render()\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        state = extract_features(next_state)\n",
    "        if reward > 0:\n",
    "            print(\n",
    "                f\"Episode {step+1}, Step {step+1}: Goal reached!\")\n",
    "            break\n",
    "        if done:\n",
    "            break  # Terminate the episode if done is True\n",
    "    if done:\n",
    "        break\n",
    "    if reward > 0:\n",
    "        break\n",
    "# for i in range(10):     frame = env.render()\n",
    "#     state = env.reset()\n",
    "#     goal = state\n",
    "#     state=extract_features(state)\n",
    "#     action = np.random.randn(env.robots[0].dof)  # sample random action\n",
    "#     # take action in the environment\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     env.render()  # render on display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
