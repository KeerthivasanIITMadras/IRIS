{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import BallReach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BallReach((640, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowLevelPolicy(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, action_dim=2, hidden_dim=128):\n",
    "        super(LowLevelPolicy, self).__init__()\n",
    "        self.rnn = nn.LSTM(state_dim + goal_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state, goal):\n",
    "        input_seq = torch.cat((state, goal), dim=-1)\n",
    "        out, _ = self.rnn(torch.unsqueeze(input_seq, 0))\n",
    "        actions = self.fc(out)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, goal_dim=4, latent_dim=20):\n",
    "        super(GoalProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, goal_dim)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    # def sample(self, num_samples, y):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(num_samples, self.num_hidden)\n",
    "    #         samples = self.decoder(self.condition_on_label(z, y))\n",
    "    #     return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.fc(torch.cat((state, action), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvaeLoss(sg, D, mu, logvar, beta=0.0001):\n",
    "    recon_loss = torch.nn.functional.mse_loss(D, sg)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProposalVAE(nn.Module):\n",
    "    def __init__(self, state_dim=4, action_dim=2, latent_dim=20):\n",
    "        super(ActionProposalVAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # self.label_projector = nn.Sequential(\n",
    "        #     nn.Linear(state_dim, latent_dim), nn.ReLU())\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        h = self.encoder(torch.cat((x, c), dim=0))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):  # P(x|z,c)\n",
    "        inputs = torch.cat([z, c], 0)\n",
    "        h3 = self.decoder(inputs)\n",
    "        return h3\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        decoded = self.decode(z, c)\n",
    "        return decoded, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\n",
    "    '/home/keerthi/IRIS/BallReach/ballreach_dataset_continous.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(dataset['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4082, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['actions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Mapping.keys of NpzFile '/home/keerthi/IRIS/BallReach/ballreach_dataset_continous.npz' with keys: observations, actions, next_observations, rewards, terminals>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_trajectories(dataset):\n",
    "    # Initialize list to store trajectories\n",
    "    trajectories = []\n",
    "    current_trajectory = {key: [] for key in dataset.keys()}\n",
    "\n",
    "    for i in range(len(dataset['rewards'])):\n",
    "        # Append each key's data for the current step\n",
    "        for key in dataset.keys():\n",
    "            current_trajectory[key].append(dataset[key][i])\n",
    "\n",
    "        # If reward is 10, finalize the current trajectory\n",
    "        if dataset['rewards'][i] == 1:\n",
    "            # Append completed trajectory to the list and reset current trajectory\n",
    "            trajectories.append(\n",
    "                {key: current_trajectory[key] for key in current_trajectory})\n",
    "            current_trajectory = {key: [] for key in dataset.keys()}\n",
    "\n",
    "    # Add any remaining steps as a final trajectory if not empty\n",
    "    if current_trajectory['rewards']:\n",
    "        trajectories.append(current_trajectory)\n",
    "\n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.56006829],\n",
       "       [-0.39010135,  0.42860776],\n",
       "       [-0.77362204,  0.53963346],\n",
       "       ...,\n",
       "       [ 0.        ,  0.33661993],\n",
       "       [ 0.73662977,  0.64769791],\n",
       "       [-0.74849565,  0.75005955]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = split_into_trajectories(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.       , 0.5560624]),\n",
       " array([0.        , 0.34249263]),\n",
       " array([0.        , 0.40938305]),\n",
       " array([0.09720208, 0.74936002]),\n",
       " array([0.7747063 , 0.71463588]),\n",
       " array([0.        , 0.39739082]),\n",
       " array([0.87617789, 0.74414573]),\n",
       " array([0.        , 0.30110621]),\n",
       " array([0.        , 0.69523896]),\n",
       " array([0.        , 0.66704971]),\n",
       " array([0.        , 0.74193264]),\n",
       " array([0.        , 0.67372962]),\n",
       " array([0.       , 0.7737488]),\n",
       " array([0.        , 0.63158808]),\n",
       " array([0.       , 0.4953053])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj[90]['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            # Set a small constant bias initialization\n",
    "            m.bias.data.fill_(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Avg Reward: -0.2427, Avg Policy Loss: 0.4163, Avg VAE Loss: 3598636337562992.0000, Avg Value Loss: 1007779327087.5797\n",
      "Iteration 20: Avg Reward: -0.2360, Avg Policy Loss: 0.3730, Avg VAE Loss: 1266233179.9719, Avg Value Loss: 1093.8010\n",
      "Iteration 30: Avg Reward: -0.3117, Avg Policy Loss: 0.2891, Avg VAE Loss: 476777.5969, Avg Value Loss: 1659.5016\n",
      "Iteration 40: Avg Reward: -0.1511, Avg Policy Loss: 0.2647, Avg VAE Loss: 463692.8438, Avg Value Loss: 1159.3336\n",
      "Iteration 50: Avg Reward: -0.2926, Avg Policy Loss: 0.2168, Avg VAE Loss: 487382.5344, Avg Value Loss: 510.9566\n",
      "Iteration 60: Avg Reward: -0.1759, Avg Policy Loss: 0.1792, Avg VAE Loss: 452874.9813, Avg Value Loss: 224.0255\n",
      "Iteration 70: Avg Reward: -0.2470, Avg Policy Loss: 0.1623, Avg VAE Loss: 442762.4281, Avg Value Loss: 242.0969\n",
      "Iteration 80: Avg Reward: -0.2393, Avg Policy Loss: 0.1452, Avg VAE Loss: 449769.5156, Avg Value Loss: 382.4279\n",
      "Iteration 90: Avg Reward: -0.1310, Avg Policy Loss: 0.1553, Avg VAE Loss: 436669.8563, Avg Value Loss: 466.5495\n",
      "Iteration 100: Avg Reward: -0.1749, Avg Policy Loss: 0.1098, Avg VAE Loss: 1722204387.7281, Avg Value Loss: 1007.3775\n",
      "Iteration 110: Avg Reward: -0.1765, Avg Policy Loss: 0.1043, Avg VAE Loss: 423740.5906, Avg Value Loss: 986.5789\n",
      "Iteration 120: Avg Reward: -0.1306, Avg Policy Loss: 0.0909, Avg VAE Loss: 427981.7406, Avg Value Loss: 954.4549\n",
      "Iteration 130: Avg Reward: -0.1962, Avg Policy Loss: 0.0938, Avg VAE Loss: 404665.5000, Avg Value Loss: 509.2045\n",
      "Iteration 140: Avg Reward: -0.3133, Avg Policy Loss: 0.0699, Avg VAE Loss: 407337.5125, Avg Value Loss: 2812.5535\n",
      "Iteration 150: Avg Reward: -0.1567, Avg Policy Loss: 0.0667, Avg VAE Loss: 408598.0969, Avg Value Loss: 1876.2192\n",
      "Iteration 160: Avg Reward: -0.2903, Avg Policy Loss: 0.0761, Avg VAE Loss: 394592.7531, Avg Value Loss: 2893.8529\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "'''\n",
    "Run this training loop for full trajecory from dataset'''\n",
    "\n",
    "\n",
    "def train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae: GoalProposalVAE, action_vae: ActionProposalVAE, value_network, trajectories, trajectory_length=5):\n",
    "    # Move models to GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    low_level_policy = low_level_policy.to(device)\n",
    "    goal_proposal_vae = goal_proposal_vae.to(device)\n",
    "    action_vae = action_vae.to(device)\n",
    "    value_network = value_network.to(device)\n",
    "\n",
    "    policy_optimizer = optim.Adam(low_level_policy.parameters(), lr=0.0001)\n",
    "    vae_optimizer = optim.Adam(goal_proposal_vae.parameters(), lr=0.001)\n",
    "    value_optimizer = optim.Adam(value_network.parameters(), lr=0.01)\n",
    "    action_optimizer = optim.Adam(action_vae.parameters(), lr=0.001)\n",
    "    M = 30\n",
    "    gamma = 0.99\n",
    "    num_trajectories = len(dataset['observations']) // trajectory_length\n",
    "\n",
    "    # Variables to store cumulative statistics\n",
    "    cumulative_rewards = []\n",
    "    cumulative_policy_loss = []\n",
    "    cumulative_vae_loss = []\n",
    "    cumulative_value_loss = []\n",
    "    iteration = 0\n",
    "    for trajectory in trajectories:\n",
    "        # Move data to GPU\n",
    "        iteration += 1\n",
    "        states = torch.tensor(\n",
    "            trajectory['observations'], dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(\n",
    "            trajectory['actions'], dtype=torch.float32).to(device)\n",
    "        rewards = torch.tensor(\n",
    "            trajectory['rewards'], dtype=torch.float32).to(device)\n",
    "        if len(rewards) == 1:\n",
    "            continue\n",
    "        actions = actions[:-1]\n",
    "        sg = states[-1]\n",
    "        s_start = states[0]\n",
    "        reward_sg = rewards[-2]\n",
    "        actionlast = actions[-2]\n",
    "        statesecondlast = states[-2]\n",
    "\n",
    "        # Train Low-Level Policy\n",
    "        policy_actions = []\n",
    "        for state in states:\n",
    "            policy_actions.append(low_level_policy(state, sg))\n",
    "        policy_actions = policy_actions[:-1]\n",
    "        policy_actions = torch.stack(policy_actions)\n",
    "        policy_actions = torch.squeeze(policy_actions)\n",
    "        policy_loss = nn.MSELoss()(policy_actions, actions)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # VAE update\n",
    "        mu, logvar = goal_proposal_vae.encode(sg, s_start)\n",
    "        z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "        vae_loss = cvaeLoss(\n",
    "            sg, goal_proposal_vae.decode(z, s_start), mu, logvar)\n",
    "\n",
    "        mua, logvara = action_vae.encode(actionlast, statesecondlast)\n",
    "        za = action_vae.reparameterize(mua, logvara)\n",
    "        actionvae_loss = cvaeLoss(actionlast, action_vae.decode(\n",
    "            za, statesecondlast), mua, logvara)\n",
    "        action_optimizer.zero_grad()\n",
    "        actionvae_loss.backward()\n",
    "        action_optimizer.step()\n",
    "\n",
    "        # Perform sampling and value update\n",
    "        sampled_actions = []\n",
    "        for _ in range(M):\n",
    "            sampled_action = action_vae.decode(za, sg)\n",
    "            sampled_actions.append(sampled_action)\n",
    "        sampled_actions = torch.stack(sampled_actions)\n",
    "\n",
    "        values = []\n",
    "        for action in sampled_actions:\n",
    "            value = value_network(sg, action)\n",
    "            values.append(value)\n",
    "        values = torch.stack(values)\n",
    "        max_value = torch.max(values)\n",
    "        Vbar = reward_sg + gamma * max_value.detach()\n",
    "        Vbar = Vbar.unsqueeze(0)\n",
    "        value_loss = nn.MSELoss()(Vbar, value_network(statesecondlast, actionlast))\n",
    "\n",
    "        # Update optimizers\n",
    "        vae_optimizer.zero_grad()\n",
    "        vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Store losses and reward\n",
    "        cumulative_rewards.append(reward_sg.item())\n",
    "        cumulative_policy_loss.append(policy_loss.item())\n",
    "        cumulative_vae_loss.append(vae_loss.item())\n",
    "        cumulative_value_loss.append(value_loss.item())\n",
    "\n",
    "        # Print averages every 1000 iterations\n",
    "        if iteration % 10 == 0 and iteration > 0:\n",
    "            avg_reward = np.mean(cumulative_rewards)\n",
    "            avg_policy_loss = np.mean(cumulative_policy_loss)\n",
    "            avg_vae_loss = np.mean(cumulative_vae_loss)\n",
    "            avg_value_loss = np.mean(cumulative_value_loss)\n",
    "\n",
    "            print(f\"Iteration {iteration}: Avg Reward: {avg_reward:.4f}, \"\n",
    "                  f\"Avg Policy Loss: {avg_policy_loss:.4f}, \"\n",
    "                  f\"Avg VAE Loss: {avg_vae_loss:.4f}, \"\n",
    "                  f\"Avg Value Loss: {avg_value_loss:.4f}\")\n",
    "\n",
    "            # Reset cumulative statistics\n",
    "            cumulative_rewards = []\n",
    "            cumulative_policy_loss = []\n",
    "            cumulative_vae_loss = []\n",
    "            cumulative_value_loss = []\n",
    "\n",
    "\n",
    "# Assuming dataset and models are already initialized\n",
    "state_dim = dataset['observations'].shape[1]\n",
    "state_goal_dim = dataset['observations'].shape[1]\n",
    "action_dim = dataset['actions'].shape[1]\n",
    "latent_dim = 8\n",
    "low_level_policy = LowLevelPolicy(state_dim, state_goal_dim, action_dim)\n",
    "goal_proposal_vae = GoalProposalVAE(state_dim, state_goal_dim, latent_dim)\n",
    "action_vae = ActionProposalVAE(state_dim, action_dim, latent_dim)\n",
    "value_network = ValueNetwork(state_dim, action_dim)\n",
    "\n",
    "# Train the IRIS algorithm using the D4RL dataset\n",
    "train_IRIS_full_trajectory(low_level_policy, goal_proposal_vae,\n",
    "                           action_vae, value_network, traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [360, 50], Action: [0.         0.73710374], Next State: [360.         123.71037395], Reward: -0.0\n",
      "State: [360.         123.71037395], Action: [0.85860199 0.60680916], Next State: [445.86019903 184.39128945], Reward: -0.8586019903069624\n",
      "State: [445.86019903 184.39128945], Action: [0.         0.72244528], Next State: [445.86019903 256.63581696], Reward: -0.0\n",
      "State: [445.86019903 256.63581696], Action: [0.         0.52177777], Next State: [445.86019903 308.8135939 ], Reward: -0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32126/3310126022.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goal_tensor = torch.tensor(goal, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [445.86019903 308.8135939 ], Action: [0.         0.36782419], Next State: [445.86019903 345.59601263], Reward: -0.0\n",
      "State: [445.86019903 345.59601263], Action: [0.         0.55045389], Next State: [445.86019903 400.64140201], Reward: -0.0\n",
      "State: [445.86019903 400.64140201], Action: [0.         0.47851148], Next State: [445.86019903 448.4925498 ], Reward: -0.0\n",
      "State: [445.86019903 448.4925498 ], Action: [-0.6843877   0.37170819], Next State: [377.42142861 480.        ], Reward: -0.6843877042376454\n",
      "State: [377.42142861 480.        ], Action: [0.         0.42143001], Next State: [377.42142861 480.        ], Reward: -0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't normalize Vector of length Zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(action)\n\u001b[0;32m---> 27\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/IRIS/BallReach/env.py:95\u001b[0m, in \u001b[0;36mBallReach.step\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     92\u001b[0m state_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(coord) \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m state]\n\u001b[1;32m     93\u001b[0m next_state_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(coord) \u001b[38;5;28;01mfor\u001b[39;00m coord \u001b[38;5;129;01min\u001b[39;00m next_state]\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove_circle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/IRIS/BallReach/env.py:107\u001b[0m, in \u001b[0;36mBallReach.move_circle\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    105\u001b[0m start_vec \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mVector2(start)\n\u001b[1;32m    106\u001b[0m end_vec \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mVector2(end)\n\u001b[0;32m--> 107\u001b[0m direction \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mend_vec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_vec\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m distance \u001b[38;5;241m=\u001b[39m start_vec\u001b[38;5;241m.\u001b[39mdistance_to(end_vec)\n\u001b[1;32m    110\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(distance\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeed)\n",
      "\u001b[0;31mValueError\u001b[0m: Can't normalize Vector of length Zero"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "state = env.state\n",
    "goal = state.copy()\n",
    "goal_proposal_vae = goal_proposal_vae.to('cpu')\n",
    "low_level_policy = low_level_policy.to('cpu')\n",
    "for step in range(1000):\n",
    "    # Render the environment and capture the frame\n",
    "    # Write frame to video\n",
    "    # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    goal_tensor = torch.tensor(goal, dtype=torch.float32)\n",
    "    # VAE update\n",
    "    state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "    mu, logvar = goal_proposal_vae.encode(goal_tensor, state_tensor)\n",
    "    z = goal_proposal_vae.reparameterize(mu, logvar)\n",
    "    goal = goal_proposal_vae.decode(z, state_tensor)\n",
    "\n",
    "    next_state = None\n",
    "    # Get the action from the low-level policy\n",
    "    reward = 0\n",
    "    for _ in range(3):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = low_level_policy(\n",
    "            state_tensor, goal).detach().numpy()\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        action = np.squeeze(action)\n",
    "        env.step(state)\n",
    "        # video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "        state = env.state\n",
    "        if reward > 0:\n",
    "            print(\n",
    "                f\"Step {step+1}: Goal reached!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
